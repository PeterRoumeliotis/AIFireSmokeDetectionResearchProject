{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMv70TA8w+ecV25641Q5gFV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterRoumeliotis/AIFireSmokeDetectionResearchProject/blob/main/AIFireAndSmokeDetectionResearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Peter Roumeliotis**"
      ],
      "metadata": {
        "id": "KULzVLqSgqod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Dense, Dropout, Input\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import kagglehub\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zvi-1Xsl4HBM",
        "outputId": "27b7df23-7cc2-46ce-c25d-b74acbb4bbcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading latest version of the dataset I am using\n",
        "path = kagglehub.dataset_download(\"brsdincer/wildfire-detection-image-data\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa7tmbqc0Q8L",
        "outputId": "c5b06fe1-230b-4d7d-e925-b944c5b2d0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/wildfire-detection-image-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "\n",
        "# Getting the source paths\n",
        "source_dir = '/kaggle/input/wildfire-detection-image-data/forest_fire/Training and Validation'\n",
        "source_fire_dir = os.path.join(source_dir, 'fire')\n",
        "source_non_fire_dir = os.path.join(source_dir, 'nofire')\n",
        "\n",
        "# Creating training and validation paths\n",
        "dest_dir = 'data'\n",
        "train_fire_dir = os.path.join(dest_dir, 'train', 'fire')\n",
        "val_fire_dir = os.path.join(dest_dir, 'validation', 'fire')\n",
        "train_non_fire_dir = os.path.join(dest_dir, 'train', 'non_fire')\n",
        "val_non_fire_dir = os.path.join(dest_dir, 'validation', 'non_fire')\n",
        "\n",
        "# Making sure the directories exist and if they dont, making them\n",
        "os.makedirs(train_fire_dir, exist_ok=True)\n",
        "os.makedirs(val_fire_dir, exist_ok=True)\n",
        "os.makedirs(train_non_fire_dir, exist_ok=True)\n",
        "os.makedirs(val_non_fire_dir, exist_ok=True)\n",
        "\n",
        "# 80% training 20% validation\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Splitting the files into each folder\n",
        "def split_data(source_folder, train_folder, val_folder, split_ratio=0.8):\n",
        "\n",
        "    # List all items in source folder and keep only files\n",
        "    file_list = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
        "    # Randomizes it\n",
        "    random.shuffle(file_list)\n",
        "\n",
        "    # Figures out at what index to split the files\n",
        "    split_point = int(len(file_list) * split_ratio)\n",
        "    train_files = file_list[:split_point]   # Training\n",
        "    val_files = file_list[split_point:]   # Validation\n",
        "\n",
        "    # Copying files into training\n",
        "    for file_name in train_files:\n",
        "        src = os.path.join(source_folder, file_name)\n",
        "        dst = os.path.join(train_folder, file_name)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "    # Copying files into validation\n",
        "    for file_name in val_files:\n",
        "        src = os.path.join(source_folder, file_name)\n",
        "        dst = os.path.join(val_folder, file_name)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "split_data(source_fire_dir, train_fire_dir, val_fire_dir, split_ratio)\n",
        "split_data(source_non_fire_dir, train_non_fire_dir, val_non_fire_dir, split_ratio)\n",
        "\n",
        "print(\"Data is split\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-dze51z4LiM",
        "outputId": "4fdb5c9e-c094-4f4a-dc6c-0c5900dd2c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is split\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 150, 150\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "train_data_dir = 'data/train'\n",
        "validation_data_dir = 'data/validation'\n",
        "\n",
        "# Augmenting training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Rescaling validation data\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Generate validation batches\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjOVd_4N4Xtx",
        "outputId": "9ea731bf-d345-45e0-a94d-44fb202c559c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2638 images belonging to 2 classes.\n",
            "Found 662 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Model"
      ],
      "metadata": {
        "id": "CpBTYw6NzERN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "QMGJigXYwhac",
        "outputId": "6fb6eaf7-a79c-4dea-d0a8-df8936456d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,785\u001b[0m (432.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,785</span> (432.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,337\u001b[0m (431.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,337</span> (431.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = validation_generator.samples // validation_generator.batch_size\n",
        "\n",
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=validation_generator, validation_steps=validation_steps)\n",
        "\n",
        "model_save_path = \"fire_detection_cnn.keras\"\n",
        "model_googledrive_path = \"/content/drive/MyDrive/fire_detection_cnn.keras\"\n",
        "model.save(model_googledrive_path)\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved to {model_save_path} and {model_googledrive_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg-TpDsn4fqE",
        "outputId": "7549bf75-e80b-4c30-997e-93e2bf07ac8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 4s/step - accuracy: 0.8247 - loss: 0.3891 - val_accuracy: 0.4313 - val_loss: 1.6181\n",
            "Epoch 2/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 774ms/step - accuracy: 0.6562 - loss: 0.4730 - val_accuracy: 0.4313 - val_loss: 1.5996\n",
            "Epoch 3/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 4s/step - accuracy: 0.8286 - loss: 0.3662 - val_accuracy: 0.5109 - val_loss: 1.3194\n",
            "Epoch 4/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 767ms/step - accuracy: 0.8438 - loss: 0.4168 - val_accuracy: 0.5250 - val_loss: 1.2966\n",
            "Epoch 5/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 4s/step - accuracy: 0.8279 - loss: 0.3701 - val_accuracy: 0.6328 - val_loss: 0.8641\n",
            "Epoch 6/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 773ms/step - accuracy: 0.9062 - loss: 0.2681 - val_accuracy: 0.6281 - val_loss: 0.8858\n",
            "Epoch 7/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 4s/step - accuracy: 0.8372 - loss: 0.3735 - val_accuracy: 0.7812 - val_loss: 0.3988\n",
            "Epoch 8/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 795ms/step - accuracy: 0.8438 - loss: 0.3934 - val_accuracy: 0.8000 - val_loss: 0.3817\n",
            "Epoch 9/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 4s/step - accuracy: 0.8528 - loss: 0.3515 - val_accuracy: 0.8859 - val_loss: 0.2953\n",
            "Epoch 10/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 764ms/step - accuracy: 0.9062 - loss: 0.3557 - val_accuracy: 0.8656 - val_loss: 0.2979\n",
            "Epoch 11/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 4s/step - accuracy: 0.8555 - loss: 0.3196 - val_accuracy: 0.8406 - val_loss: 0.3477\n",
            "Epoch 12/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 756ms/step - accuracy: 0.9375 - loss: 0.2412 - val_accuracy: 0.8062 - val_loss: 0.4120\n",
            "Epoch 13/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 4s/step - accuracy: 0.8676 - loss: 0.3129 - val_accuracy: 0.8781 - val_loss: 0.2753\n",
            "Epoch 14/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 783ms/step - accuracy: 0.9375 - loss: 0.2101 - val_accuracy: 0.8672 - val_loss: 0.2822\n",
            "Epoch 15/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 4s/step - accuracy: 0.8519 - loss: 0.3281 - val_accuracy: 0.8594 - val_loss: 0.3176\n",
            "Epoch 16/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 770ms/step - accuracy: 0.9688 - loss: 0.1936 - val_accuracy: 0.8594 - val_loss: 0.3137\n",
            "Epoch 17/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 4s/step - accuracy: 0.8575 - loss: 0.3363 - val_accuracy: 0.8922 - val_loss: 0.2989\n",
            "Epoch 18/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 791ms/step - accuracy: 0.8438 - loss: 0.2629 - val_accuracy: 0.8953 - val_loss: 0.2847\n",
            "Epoch 19/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 4s/step - accuracy: 0.8579 - loss: 0.3305 - val_accuracy: 0.7094 - val_loss: 0.6170\n",
            "Epoch 20/20\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 797ms/step - accuracy: 0.8438 - loss: 0.2470 - val_accuracy: 0.7188 - val_loss: 0.6044\n",
            "Model saved to fire_detection_cnn.keras and /content/drive/MyDrive/fire_detection_cnn.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics --quiet"
      ],
      "metadata": {
        "id": "kPIEF1DJv4bw",
        "outputId": "35d9c124-cc2a-4ec1-bbb3-d0ba2c10e701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/\n",
        "# https://www.digitalocean.com/community/tutorials/train-yolov5-custom-data\n",
        "# Sources I used to help me format my dataset for yolo\n",
        "\n",
        "for split in [\"train\",\"validation\"]:\n",
        "    for cls_idx, cls in enumerate([\"non_fire\",\"fire\"]):\n",
        "        folder = f\"data/{split}/{cls}\"\n",
        "        for img in os.listdir(folder):\n",
        "            # Skips non images\n",
        "            if not img.lower().endswith((\".jpg\",\".png\")): continue\n",
        "            img_path   = os.path.join(folder, img)\n",
        "            label_path = os.path.splitext(img_path)[0] + \".txt\"\n",
        "\n",
        "            if cls == \"fire\":\n",
        "                with open(label_path, \"w\") as f:\n",
        "                    # Formats data for yolo\n",
        "                    f.write(f\"{1} 0.5 0.5 1.0 1.0\\n\")\n",
        "            else:\n",
        "                open(label_path, \"w\").close()\n"
      ],
      "metadata": {
        "id": "8A9iHkLYAb31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making YAML for yolo\n",
        "\n",
        "# https://docs.ultralytics.com/\n",
        "# https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/\n",
        "\n",
        "# Used these sources for help\n",
        "\n",
        "%%bash\n",
        "cat <<EOF > data/fire_data.yaml\n",
        "train: /content/data/train\n",
        "val:   /content/data/validation\n",
        "\n",
        "names:\n",
        "  0: non_fire\n",
        "  1: fire\n",
        "EOF\n",
        "\n",
        "echo \"Created YAML:\"\n",
        "cat data/fire_data.yaml\n"
      ],
      "metadata": {
        "id": "c3_4wD-twKV4",
        "outputId": "3aef08f1-50e0-4938-e78d-7127381b6d3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created YAML:\n",
            "train: /content/data/train\n",
            "val:   /content/data/validation\n",
            "\n",
            "names:\n",
            "  0: non_fire\n",
            "  1: fire\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "yolo.train(\n",
        "    data='data/fire_data.yaml',\n",
        "    epochs=20,\n",
        "    imgsz=150,\n",
        "    batch=32,\n",
        "    project='yolo-fire',\n",
        "    name='exp'\n",
        ")\n",
        "\n",
        "yolo_model = YOLO('yolo-fire/exp/weights/best.pt')\n",
        "yolo_model.save('/content/drive/MyDrive/best.pt')\n"
      ],
      "metadata": {
        "id": "9f2AG7tcwNnD",
        "outputId": "50e6e932-1cd1-4eaa-a79d-acd2b5c57017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 73.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.128 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data/fire_data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=150, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=yolo-fire, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=yolo-fire/exp, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 13.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 62.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "WARNING ⚠️ imgsz=[150] must be multiple of max stride 32, updating to [160]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1220.9±565.8 MB/s, size: 438.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data/train/fire... 2638 images, 1516 backgrounds, 0 corrupt: 100%|██████████| 2638/2638 [00:01<00:00, 1766.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/data/train/fire/42848957940_cc5924c5ba_o.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/data/train/fire/45600718572_9a9d934a92_o.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/data/train/fire/50623063192_ec4f7f56b2_o.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/data/train/non_fire/39728610281_68e77869f2_o.jpg: corrupt JPEG restored and saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data/train/fire.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1456.5±2071.5 MB/s, size: 1988.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data/validation/fire... 662 images, 380 backgrounds, 0 corrupt: 100%|██████████| 662/662 [00:00<00:00, 1623.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/data/validation/fire.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to yolo-fire/exp/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 160 train, 160 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1myolo-fire/exp\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/20     0.346G     0.4735      1.596      1.033         15        160: 100%|██████████| 83/83 [00:54<00:00,  1.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:03<00:00,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.743       0.72      0.771      0.449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/20     0.426G     0.3305     0.7862     0.9488         26        160: 100%|██████████| 83/83 [00:43<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.876      0.901      0.906       0.87\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/20     0.443G     0.3047     0.6914     0.9334         24        160: 100%|██████████| 83/83 [00:39<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.487      0.833      0.583      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/20     0.459G      0.301       0.68     0.9401         17        160: 100%|██████████| 83/83 [00:42<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282       0.93      0.915      0.966       0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/20     0.477G      0.275     0.6168     0.9273         25        160: 100%|██████████| 83/83 [00:43<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.593      0.706      0.689      0.635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/20     0.492G     0.2467     0.5655     0.9234         23        160: 100%|██████████| 83/83 [00:41<00:00,  2.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.927      0.617      0.721      0.656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/20      0.51G     0.2154      0.521     0.9089         22        160: 100%|██████████| 83/83 [00:46<00:00,  1.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.875      0.908      0.925      0.878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/20     0.527G     0.2066     0.5139     0.9137         18        160: 100%|██████████| 83/83 [00:45<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:01<00:00,  5.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.839      0.911      0.917      0.816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/20     0.545G      0.195     0.4858     0.9127         14        160: 100%|██████████| 83/83 [00:43<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.918      0.911      0.938      0.925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/20     0.561G     0.1812     0.4483     0.9024         18        160: 100%|██████████| 83/83 [00:38<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.976      0.901      0.959      0.923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/20     0.578G     0.1492     0.4791     0.9089          6        160: 100%|██████████| 83/83 [00:45<00:00,  1.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.934      0.918      0.971      0.965\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/20     0.594G      0.128     0.3096     0.8966          4        160: 100%|██████████| 83/83 [00:40<00:00,  2.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  3.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.985      0.957      0.989      0.988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/20     0.611G      0.112     0.2974     0.8977          3        160: 100%|██████████| 83/83 [00:42<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  3.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.985      0.945      0.991       0.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/20     0.629G     0.1026     0.2576     0.8987          5        160: 100%|██████████| 83/83 [00:43<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  5.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.961      0.961      0.988      0.978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/20     0.646G     0.1019     0.2786      0.888          5        160: 100%|██████████| 83/83 [00:42<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.948       0.94      0.977      0.975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/20     0.662G    0.08346     0.2213     0.8804          6        160: 100%|██████████| 83/83 [00:44<00:00,  1.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  5.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.954      0.926      0.986      0.983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/20      0.68G    0.08212     0.2256     0.8814          7        160: 100%|██████████| 83/83 [00:42<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:01<00:00,  6.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.985      0.961      0.988      0.985\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/20     0.695G    0.07603     0.1837     0.8847          4        160: 100%|██████████| 83/83 [00:43<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:01<00:00,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.978      0.961      0.989      0.989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/20     0.713G    0.06885     0.1599     0.8757          6        160: 100%|██████████| 83/83 [00:41<00:00,  2.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.983       0.95      0.991      0.991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/20      0.73G    0.06048     0.1536     0.8731          7        160: 100%|██████████| 83/83 [00:41<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:01<00:00,  5.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.977      0.965      0.991       0.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "20 epochs completed in 0.259 hours.\n",
            "Optimizer stripped from yolo-fire/exp/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from yolo-fire/exp/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating yolo-fire/exp/weights/best.pt...\n",
            "Ultralytics 8.3.128 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:03<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        662        282      0.985       0.95      0.992      0.991\n",
            "                  fire        282        282      0.985       0.95      0.992      0.991\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1myolo-fire/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = tf.keras.models.load_model('fire_detection_cnn.keras')\n",
        "yolo_model = YOLO('yolo-fire/exp/weights/best.pt')\n",
        "\n",
        "# Upload/open vid\n",
        "uploaded = files.upload()\n",
        "video_path = next(iter(uploaded.keys()))\n",
        "print(f\"Video file: {video_path}\")\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Frames per second, default is 30\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "\n",
        "frame_indices = []  # Frame numbers\n",
        "cnn_flags     = []  # My model\n",
        "yolo_flags    = []  # Yolo\n",
        "\n",
        "frame_idx = 0\n",
        "\n",
        "# Loop through vid by frame\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_idx += 1\n",
        "\n",
        "    # My model detection\n",
        "    # Resize/normalize\n",
        "    img = cv2.resize(frame, (150,150)) / 255.0\n",
        "    # Get the fire probability from my model\n",
        "    p = cnn_model.predict(np.expand_dims(img,0), verbose=0)[0][0]\n",
        "    # Detect if probability > 0.5\n",
        "    cnn_detected = (p > 0.5)\n",
        "\n",
        "    # YOLO detection\n",
        "    # Run yolo\n",
        "    res = yolo_model(frame, verbose=False)[0]\n",
        "    classes = res.boxes.cls.cpu().numpy().astype(int).tolist()\n",
        "    # Detect if there is class 1 (fire)\n",
        "    yolo_detected = (1 in classes)\n",
        "\n",
        "    # Results\n",
        "    frame_indices.append(frame_idx)\n",
        "    cnn_flags.append(cnn_detected)\n",
        "    yolo_flags.append(yolo_detected)\n",
        "\n",
        "cap.release()\n"
      ],
      "metadata": {
        "id": "z2-L6mgayG04",
        "outputId": "cfdb9abd-0450-49d3-a9db-e7728f4da0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1f49a4eb-de52-477c-9b52-988eda2aada0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1f49a4eb-de52-477c-9b52-988eda2aada0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving YosemiteForestFire.mp4 to YosemiteForestFire.mp4\n",
            "Video file: YosemiteForestFire.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cnn_frames = [frame_indices[i] for i, flag in enumerate(cnn_flags) if flag]\n",
        "yolo_frames = [frame_indices[i] for i, flag in enumerate(yolo_flags) if flag]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "# My model detections (red circles)\n",
        "ax1.scatter(cnn_frames, [1]*len(cnn_frames),\n",
        "            label='My model detected', marker='o', c='red')\n",
        "ax1.set_yticks([1])\n",
        "ax1.set_yticklabels(['My model fire'])\n",
        "ax1.set_title('My model per‐frame detections')\n",
        "\n",
        "# Yolo detections (blue X’s)\n",
        "ax2.scatter(yolo_frames, [1]*len(yolo_frames),\n",
        "            label='Yolo detected', marker='x', c='blue')\n",
        "ax2.set_yticks([1])\n",
        "ax2.set_yticklabels(['Yolo fire'])\n",
        "ax2.set_title('Yolo per‐frame detections')\n",
        "\n",
        "ax2.set_xlabel('Frame Number')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UMgiH6PfjkaU",
        "outputId": "a02dce5e-6ae9-4387-ce9d-e0345a9d30b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJOCAYAAABm7rQwAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXh5JREFUeJzt3XmcjfX///HnmX2fsY11GIbsO0mqUckkskQ+dqJSCBVKi6XSpPqkKKlPhcSnUsQHCUWLr60s2RKyZRvbzJjQMPP+/XF+c3LMDIOZ95nhcb/dzs2c9/U+1/t1XXNd15zzdF3XcRhjjAAAAAAAAACLvDxdAAAAAAAAAK4/hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAA4JrVq1cvRUdHX9FrmzZtqqZNm+ZqPXlpzZo1uvnmmxUcHCyHw6H169d7uqQ8FR0drV69enm6jCtyNdslAADXEkIpAACuA1OmTJHD4ZDD4dBPP/2UaboxRlFRUXI4HGrVqpUHKsTVOHv2rO6//34dP35c48aN07Rp01SuXDlPl5UvnTp1SqNGjdKyZcvydJwDBw5o1KhR13w4CADA1fDxdAEAAMCegIAAzZgxQ7fccotb+/fff68///xT/v7+HqoMV2Pnzp3as2eP/vOf/+jBBx/0dDn52qlTpzR69GhJytMz4Q4cOKDRo0crOjpaderUcZv2n//8R+np6Xk2NgAABQVnSgEAcB255557NHPmTJ07d86tfcaMGapfv75KlCjhocpwMWfOnLloiJGQkCBJioiIuOS8/vrrr9wqC1fI19eXABgAABFKAQBwXencubOOHTumxYsXu9pSU1P1xRdfqEuXLm59jTGKjo5WmzZtMs3nzJkzCg8PV9++fS86nsPh0IABAzRz5kxVq1ZNgYGBaty4sTZu3ChJeu+991SxYkUFBASoadOm2r17d6Z5zJw5U/Xr11dgYKCKFi2qbt26af/+/Zn6ffXVV6pRo4YCAgJUo0YNzZ49O8ua0tPT9eabb6p69eoKCAhQ8eLF1bdvX504ceKiy3KpZZw+fboqV66sgIAA1a9fXz/88EOmvvv371fv3r1VvHhx+fv7q3r16vroo4/c+ixbtkwOh0OffvqpnnvuOZUuXVpBQUFKTk7OcvxevXopNjZWknT//ffL4XC4zgDq1auXQkJCtHPnTt1zzz0KDQ1V165dJUk//vij7r//fpUtW1b+/v6KiorS448/rtOnT2eaf0hIiPbu3atWrVopJCREpUuX1jvvvCNJ2rhxo+644w4FBwerXLlymjFjRqYaExMTNXjwYEVFRcnf318VK1bU2LFjc3S2kDFGL730ksqUKaOgoCDdfvvt2rx5c5Z9LzXO7t27VaxYMUnS6NGjXZe0jho1yjWP3377TR06dFDhwoUVEBCgBg0aaO7cuVmO9fjjjys6Olr+/v4qU6aMevTooaNHj2rZsmVq2LChJOmBBx5wjTNlyhTXOr3wnlJ//fWXnnzySVftlStX1uuvvy5jjFu/jO0tY3vP2I4WLlzo1u/kyZMaPHiwq77IyEjdddddWrt27SXXOQAAtnD5HgAA15Ho6Gg1btxY//3vf9WiRQtJ0tdff62kpCR16tRJ48ePd/V1OBzq1q2bXn31VR0/flyFCxd2Tfvf//6n5ORkdevW7ZJj/vjjj5o7d6769+8vSYqPj1erVq00bNgwTZw4Uf369dOJEyf06quvqnfv3vruu+9cr50yZYoeeOABNWzYUPHx8Tp8+LDeeustLV++XOvWrXOdGbRo0SK1b99e1apVU3x8vI4dO6YHHnhAZcqUyVRP3759XfMdOHCgdu3apbffflvr1q3T8uXL5evre9nr9fvvv9dnn32mgQMHyt/fXxMnTtTdd9+t1atXq0aNGpKkw4cP66abbnKFCsWKFdPXX3+tPn36KDk5WYMHD3ab54svvig/Pz8NGTJEf//9t/z8/LIcu2/fvipdurRefvllDRw4UA0bNlTx4sVd08+dO6e4uDjdcsstev311xUUFCTJGfadOnVKjz76qIoUKaLVq1drwoQJ+vPPPzVz5ky3MdLS0tSiRQvddtttevXVVzV9+nQNGDBAwcHBevbZZ9W1a1fdd999mjRpknr06KHGjRurfPnykpyXy8XGxmr//v3q27evypYtq//7v//T8OHDdfDgQb355psXXbcjRozQSy+9pHvuuUf33HOP1q5dq+bNmys1NdWtX07GKVasmN599109+uijateune677z5JUq1atSRJmzdvVpMmTVS6dGk9/fTTCg4O1ueff662bdvqyy+/VLt27SRJKSkpuvXWW7V161b17t1b9erV09GjRzV37lz9+eefqlq1ql544QWNGDFCDz/8sG699VZJ0s0335zlMhpj1Lp1ay1dulR9+vRRnTp19M0332jo0KHav3+/xo0b59b/p59+0qxZs9SvXz+FhoZq/Pjxat++vfbu3asiRYpIkh555BF98cUXGjBggKpVq6Zjx47pp59+0tatW1WvXr2LrnMAAKwxAADgmjd58mQjyaxZs8a8/fbbJjQ01Jw6dcoYY8z9999vbr/9dmOMMeXKlTMtW7Z0vW7btm1Gknn33Xfd5te6dWsTHR1t0tPTLzquJOPv72927drlanvvvfeMJFOiRAmTnJzsah8+fLiR5OqbmppqIiMjTY0aNczp06dd/ebNm2ckmREjRrja6tSpY0qWLGkSExNdbYsWLTKSTLly5VxtP/74o5Fkpk+f7lbnwoULM7XHxsaa2NjYiy5fxjJKMj///LOrbc+ePSYgIMC0a9fO1danTx9TsmRJc/ToUbfXd+rUyYSHh7t+H0uXLjWSTIUKFVxtl5LxmpkzZ7q19+zZ00gyTz/9dKbXZDXv+Ph443A4zJ49ezLN4+WXX3a1nThxwgQGBhqHw2E+/fRTV/tvv/1mJJmRI0e62l588UUTHBxsfv/9d7exnn76aePt7W327t2b7XIlJCQYPz8/07JlS7dt7ZlnnjGSTM+ePS97nCNHjmSqMcOdd95patasac6cOeNqS09PNzfffLOpVKmSq23EiBFGkpk1a1ameWTUuWbNGiPJTJ48OVOfnj17um2XX331lZFkXnrpJbd+HTp0MA6Hw+zYscPVJsn4+fm5tW3YsMFIMhMmTHC1hYeHm/79+2caGwCA/ITL9wAAuM507NhRp0+f1rx583Ty5EnNmzcv06V7GW644QY1atRI06dPd7UdP35cX3/9tbp27SqHw3HJ8e688063S5UaNWokSWrfvr1CQ0Mztf/xxx+SpJ9//lkJCQnq16+fAgICXP1atmypKlWqaP78+ZKkgwcPav369erZs6fCw8Nd/e666y5Vq1bNrZaZM2cqPDxcd911l44ePep61K9fXyEhIVq6dOkllycrjRs3Vv369V3Py5YtqzZt2uibb75RWlqajDH68ssvde+998oY4zZ2XFyckpKSMl1W1bNnTwUGBl5RPRd69NFHM7WdP++//vpLR48e1c033yxjjNatW5ep//k3UI+IiFDlypUVHBysjh07utorV66siIgI1+9Qcq7zW2+9VYUKFXJb7mbNmiktLS3LyxwzLFmyRKmpqXrsscfctrULzyq72nEk53b93XffqWPHjjp58qTr9ceOHVNcXJy2b9/uumz0yy+/VO3atV1nTp0vJ/vEhRYsWCBvb28NHDjQrf3JJ5+UMUZff/21W3uzZs0UExPjel6rVi2FhYW5rfeIiAitWrVKBw4cuOx6AACwhcv3AAC4zhQrVkzNmjXTjBkzdOrUKaWlpalDhw7Z9u/Ro4cGDBigPXv2qFy5cpo5c6bOnj2r7t2752i8smXLuj3PCI6ioqKybM+4t9OePXskOYOOC1WpUkU//fSTW79KlSpl6le5cmW3sGf79u1KSkpSZGRklrVm3DD8cmU19g033KBTp07pyJEj8vLyUmJiot5//329//77ORo74/K3DGlpaTpy5IhbW+HChbO9rC+Dj49Plpcx7t27VyNGjNDcuXMz3U8rKSnJ7XlAQIDrXkwZwsPDVaZMmUwhTHh4uNv8tm/frl9//TXT6zNcbJ1n97stVqyYChUq5NZ2NeNI0o4dO2SM0fPPP6/nn38+23mULl1aO3fuVPv27S86v8uxZ88elSpVyi2klaSqVau6pp/vwn1KkgoVKuS23l999VX17NlTUVFRql+/vu655x716NFDFSpUyLW6AQC4WoRSAABch7p06aKHHnpIhw4dUosWLS76rW2dOnXS448/runTp+uZZ57RJ598ogYNGmQZFmXF29v7strNBTd2zk3p6emKjIx0O/PrfNkFGrkxriR169ZNPXv2zLJPxn2NMlx4ltS+ffsyBVVLly513dQ8O/7+/vLycj85Pi0tTXfddZeOHz+up556SlWqVFFwcLD279+vXr16ZboB+dX8DtPT03XXXXdp2LBhWfa94YYbLlp/Tl3tOBnLPGTIEMXFxWXZp2LFildXZC7JyXrv2LGjbr31Vs2ePVuLFi3Sa6+9prFjx2rWrFmu+8kBAOBphFIAAFyH2rVrp759+2rlypX67LPPLtq3cOHCatmypaZPn66uXbtq+fLll7w5dW4oV66cJGnbtm2644473KZt27bNNT3j3+3bt2eax7Zt29yex8TEaMmSJWrSpEmuXRqX3di///67goKCXEFXaGio0tLS1KxZsysao0SJEm7fmihJtWvXvqJ5bdy4Ub///rumTp2qHj16uNovnH9uiImJUUpKyhUt9/m/2/PP8Dly5Eims7tyOk52l9dlzN/X1/eS84iJidGmTZuuaJyslCtXTkuWLNHJkyfdzpb67bffXNOvRMmSJdWvXz/169dPCQkJqlevnsaMGUMoBQDIN7inFAAA16GQkBC9++67GjVqlO69995L9u/evbu2bNmioUOHytvbW506dcrzGhs0aKDIyEhNmjRJf//9t6v966+/1tatW9WyZUtJzg/ederU0dSpU90uO1u8eLG2bNniNs+OHTsqLS1NL774Yqbxzp07p8TExCuqdcWKFW6XCe7bt09z5sxR8+bN5e3tLW9vb7Vv315ffvlllmHGhZflZSUgIEDNmjVze1x4CVtOZZxpc/6ZNcYYvfXWW1c0v4vp2LGjVqxYoW+++SbTtMTERJ07dy7b1zZr1ky+vr6aMGGCW61ZhaI5HSfj2wcv/F1HRkaqadOmeu+993Tw4MFM8zj/d9S+fXtt2LBBs2fPztQvo87g4OAsx8nKPffco7S0NL399ttu7ePGjZPD4bjsECktLS3TJZiRkZEqVaqU274EAICncaYUAADXqewuI8tKy5YtVaRIEc2cOVMtWrTI9p5MucnX11djx47VAw88oNjYWHXu3FmHDx/WW2+9pejoaD3++OOuvvHx8WrZsqVuueUW9e7dW8ePH9eECRNUvXp1paSkuPrFxsaqb9++io+P1/r169W8eXP5+vpq+/btmjlzpt56662L3l8rOzVq1FBcXJwGDhwof39/TZw4UZI0evRoV59XXnlFS5cuVaNGjfTQQw+pWrVqOn78uNauXaslS5bo+PHjV7G2Lk+VKlUUExOjIUOGaP/+/QoLC9OXX36Z6eyj3DB06FDNnTtXrVq1Uq9evVS/fn399ddf2rhxo7744gvt3r1bRYsWzfK1xYoV05AhQxQfH69WrVrpnnvu0bp16/T1119nek1OxwkMDFS1atX02Wef6YYbblDhwoVVo0YN1ahRQ++8845uueUW1axZUw899JAqVKigw4cPa8WKFfrzzz+1YcMG11hffPGF7r//fvXu3Vv169fX8ePHNXfuXE2aNEm1a9dWTEyMIiIiNGnSJIWGhio4OFiNGjXKdAmmJN177726/fbb9eyzz2r37t2qXbu2Fi1apDlz5mjw4MFuNzXPiZMnT6pMmTLq0KGDateurZCQEC1ZskRr1qzRv//978uaFwAAeYlQCgAAXJKfn5/+9a9/aeLEiTm+wXlu6NWrl4KCgvTKK6/oqaeeUnBwsNq1a6exY8e63Qfr7rvv1syZM/Xcc89p+PDhiomJ0eTJkzVnzhwtW7bMbZ6TJk1S/fr19d577+mZZ56Rj4+PoqOj1a1bNzVp0uSK6oyNjVXjxo01evRo7d27V9WqVdOUKVPc7hNVvHhxrV69Wi+88IJmzZqliRMnqkiRIqpevbrGjh17ReNeKV9fX/3vf//TwIEDFR8fr4CAALVr104DBgy44ksCsxMUFKTvv/9eL7/8smbOnKmPP/5YYWFhuuGGGzR69Gi3b0zMyksvvaSAgABNmjTJFeotWrTIdabclYzzwQcf6LHHHtPjjz+u1NRUjRw5UjVq1FC1atX0888/a/To0ZoyZYqOHTumyMhI1a1bVyNGjHC9PiQkRD/++KNGjhyp2bNna+rUqYqMjNSdd97puqm8r6+vpk6dquHDh+uRRx7RuXPnNHny5CxDKS8vL82dO1cjRozQZ599psmTJys6OlqvvfaannzyySta5/369dOiRYs0a9Yspaenq2LFipo4cWKW38QIAICnOExe3k0UAABcMx5//HF9+OGHOnTokOsSKDjvHdS/f/9Ml14BAADg4rinFAAAuKQzZ87ok08+Ufv27QmkAAAAkCu4fA8AAGQrISFBS5Ys0RdffKFjx45p0KBBni4JAAAA1whCKQAAkK0tW7aoa9euioyM1Pjx41WnTh1PlwQAAIBrBPeUAgAAAAAAgHXcUwoAAAAAAADWEUoBAAAAAADAOu4pdY1IT0/XgQMHFBoaKofD4elyAAAAAABAAWGM0cmTJ1WqVCl5edk7f4lQ6hpx4MABRUVFeboMAAAAAABQQO3bt09lypSxNh6h1DUiNDRUknMDCgsL83A1AAAAAACgoEhOTlZUVJQrW7CFUOoakXHJXlhYGKEUAAAAAAC4bLZvB8SNzgEAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1120otXv3bjkcDq1fvz7Hr2natKkGDx580T7vv/++oqKi5OXlpTfffFOjRo1SnTp1rqpWAAAAAACAa81lhVK9evWSw+HQI488kmla//795XA41KtXr9yqrcBJTk7WgAED9NRTT2n//v16+OGHNWTIEH377beeLi3/Sk2VnntOcjgK7qNYMenQIefy7N0reXt7vqacPBo0kJKSPPv7B65Gaqr05pvSY485/01N9XRFeS8pSapXL/v9OjBQeuEF57pISZHuvVcKDZXCwqRWrZxtKSlSu3ZSrVrOf1NSnPNOSZHatJFKlpQqVJDGjs3ZOj1+XKpa9eqPn56wf78UEJDzer28pB49pNOnPVfzpbYB/g7gWpKSIsXFef49Ew8ePK6fh4+PNGLE5b2vvPAzYH54v1CAOIwxJqede/Xqpe+++07Jyck6ePCgAgMDJUlnzpxRyZIlFRYWpttvv11TpkzJq3pzze7du1W+fHmtW7cux2cyNW3aVHXq1NGbb76Z5fRNmzapZs2a+uOPP1S+fPkczTM1NVV+fn45rDp7ycnJCg8PV1JSksLCwq56flYMGya99pqnq0BMjLRjh6erAC7PsGHSG29IaWn/tHl7S088Ib36qufqyksVK0o7d+bNvIOCpFOnsp42dGj267RECenw4dwZ/6+/rn4+l8Pf/+qCzDZtpK++yrVyciSvtgH+DiA/uvFGac0aT1cB4Hp2sfdAGXx9pXPnsp/uifcLV8hTmcJlX75Xr149RUVFadasWa62WbNmqWzZsqpbt66r7eOPP1aRIkX0999/u72+bdu26t69e5bzzrik7vPPP9ett96qwMBANWzYUL///rvWrFmjBg0aKCQkRC1atNCRI0dcr0tPT9cLL7ygMmXKyN/fX3Xq1NHChQvd5r169WrVrVtXAQEBatCggdatW5dp/E2bNqlFixYKCQlR8eLF1b17dx09ejRH62XKlCmqWbOmJKlChQpyOBzavXt3psv3evXqpbZt22rMmDEqVaqUKleuLEnat2+fOnbsqIiICBUuXFht2rTR7t27czR2gUQglX/s3On8oAMUFBnHj/MDKcn5/LXXnNOvNXkZSEnZB1JS9us0twKpjPGDg3NnXjlxtYGUJM2ZI7Vtmyvl5EhebgP8HUB+QyAFID+41PvKSwVSkv33CwXQFd1Tqnfv3po8ebLr+UcffaQHHnjArc/999+vtLQ0zZ0719WWkJCg+fPnq3fv3hed/8iRI/Xcc89p7dq18vHxUZcuXTRs2DC99dZb+vHHH7Vjxw6NGDHC1f+tt97Sv//9b73++uv69ddfFRcXp9atW2v79u2SpJSUFLVq1UrVqlXTL7/8olGjRmnIkCFuYyYmJuqOO+5Q3bp19fPPP2vhwoU6fPiwOnbsmKN18q9//UtLliyR5AzADh48qKioqCz7fvvtt9q2bZsWL16sefPm6ezZs4qLi1NoaKh+/PFHLV++XCEhIbr77ruVei1ejpKaSiCV3+zcySUcKBhSU51nSF3MG29cW5fyJSXlbSCVExeu0+PHcy+QynDqlJ1L+fbvz73tY84cO6fm29gG+DuA/CIlhUAKQP7x739n/b5h795LB1IZbL1fKKCuKJTq1q2bfvrpJ+3Zs0d79uzR8uXL1a1bN7c+gYGB6tKli1t49cknn6hs2bJq2rTpRec/ZMgQxcXFqWrVqho0aJB++eUXPf/882rSpInq1q2rPn36aOnSpa7+r7/+up566il16tRJlStX1tixY90us5sxY4bS09P14Ycfqnr16mrVqpWGDh3qNubbb7+tunXr6uWXX1aVKlVUt25dffTRR1q6dKl+//33S66TwMBAFSlSRJJUrFgxlShRQt7e3ln2DQ4O1gcffKDq1aurevXq+uyzz5Senq4PPvhANWvWVNWqVTV58mTt3btXy5Yty3Ief//9t5KTk90eBcbEiZ6uAFlp2dLTFQCXNnFi5jOkLpSWdm0dZ/LDvnnhOo2NzZtxbHwxyP8/qznXXPB+Ik/Y2gbyw7YGZHNFBQB4RHp61u8rq1e/vPnYeL9QQPlcyYuKFSumli1basqUKTLGqGXLlipatGimfg899JAaNmyo/fv3q3Tp0poyZYrrZukXU6tWLdfPxYsXlyTXpXEZbQkJCZKc1z0eOHBATZo0cZtHkyZNtGHDBknS1q1bVatWLQUEBLimN27c2K3/hg0btHTpUoWEhGSqZ+fOnbrhhhsuWvPlqFmzptt9pDZs2KAdO3YoNDTUrd+ZM2e0M5v/GY2Pj9fo0aNzrSarPP0//sja3r2ergC4tJweP66l40x+2TfPX6cHDuTNGImJeTPf8508mbvz+/9nZecpW9tAftnWcH27lo7fAK4NWR2XLnbrg6zYeL9QQF1RKCU5L+EbMGCAJOmdd97Jsk/dunVVu3Ztffzxx2revLk2b96s+fPnX3Levr6+rp8zAqwL29LT06+09CylpKTo3nvv1dixYzNNK1myZK6OFXzBfTNSUlJUv359TZ8+PVPfYsWKZTmP4cOH64knnnA9T05OzvZywXwnJsbTFSArZct6ugLg0nJ6/LiWjjNly0r79nm6Cvd1WqqU8xK+3BYRkfvzvFBoqHTiRO7Nr1Kl3JtXdmxtA/wdQH4QEyNt3OjpKgDgH1m9rwwK+ucbjHPCxvuFAuqKLt+T5LrfUcb9kLLz4IMPasqUKZo8ebKaNWuW68FJWFiYSpUqpeXLl7u1L1++XNWqVZMkVa1aVb/++qvOnDnjmr5y5Uq3/vXq1dPmzZsVHR2tihUruj0uDJFyW7169bR9+3ZFRkZmGjs8PDzL1/j7+yssLMztUWD06+fpCpCVHATGgMf16+f8lr2L8fa+to4z+WHfvHCdfv993oyzfn3ezPd8uf1h18Y9Em1tA/lhWwOmTfN0BQDwDy+vrN9Xbt58efPhnsrZuuJQytvbW1u3btWWLVuyvXeSJHXp0kV//vmn/vOf/1zyBudXaujQoRo7dqw+++wzbdu2TU8//bTWr1+vQYMGuWpwOBx66KGHtGXLFi1YsECvv/662zz69++v48ePq3PnzlqzZo127typb775Rg888IDSLnX/kqvUtWtXFS1aVG3atNGPP/6oXbt2admyZRo4cKD+/PPPPB3bI/z8uKY2v4mJkbIJQIF8xc9POu8s0Sw98YSz37UiPNzzZ35duE4LF5b+/+X1uSYoyPmNfnmtdOnc2z7atJECA3NnXhdjYxvg7wDyi5AQqWFDT1cBAE5PPpn1+4ayZSWfHF54Zuv9QgF1xaGUpBydoRMeHq727dsrJCREbfPoqxAHDhyoJ554Qk8++aRq1qyphQsXau7cuar0/0+RCwkJ0f/+9z9t3LhRdevW1bPPPpvpMr2Ms63S0tLUvHlz1axZU4MHD1ZERIS8vK5qNV1SUFCQfvjhB5UtW1b33Xefqlatqj59+ujMmTMF6wyoy/HqqwRT+UVMjLRjh6erAHIu4/hx4X+IeHs721991TN15aUdO/I2lAgKyn5aduv00KHcC6aCgqS//sqdeeXE339ffTDVpo301Ve5Uk6O5OU2wN8B5DerVxNMAfC8S72vPHv20sGU7fcLBZDDGGPyepA777xT1atX1/jx4/N6qOtWcnKywsPDlZSUVLCCrNRU6YUXpDFjPF3JlSta1Hk5SIkSzpvEli/v/JaG/K5+fenbb/mfcRRcqanOb0PZudP5obpfv2vrDKmsJCVJt98urVuX9fSAAGn4cOnpp53rp3NnadkyyeGQbrtN+vRTZ7/u3f9Zb9OmOc9MSEmRunZ1fhgMDJT69pUef/zS6/T4calJE+m33y5/ec4/fnrC/v3OdfD33znr73BI3bpJ773nuf/xvNQ2cDn4O4D8LiVFat9eWrTI05UAuF54e0vPPCM991zO31de+BkwP7xfuAKeyhTyNJQ6ceKEli1bpg4dOmjLli2qXLlyXg113SuwoRQAAAAAAPAoT2UKV/ztezlRt25dnThxQmPHjiWQAgAAAAAAgEuehlK7d+/Oy9kDAAAAAACggMrbO3gDAAAAAAAAWSCUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACs8/F0AcgdxhhJUnJysocrAQAAAAAABUlGlpCRLdhCKHWNOHnypCQpKirKw5UAAAAAAICC6OTJkwoPD7c2nsPYjsGQJ9LT03XgwAGFhobK4XB4upzLkpycrKioKO3bt09hYWGeLgfIN9g3gKyxbwDZY/8Assa+AWQtY9/Yu3evHA6HSpUqJS8ve3d64kypa4SXl5fKlCnj6TKuSlhYGH8ggCywbwBZY98Assf+AWSNfQPIWnh4uEf2DW50DgAAAAAAAOsIpQAAAAAAAGAdoRQ8zt/fXyNHjpS/v7+nSwHyFfYNIGvsG0D22D+ArLFvAFnz9L7Bjc4BAAAAAABgHWdKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAD5xpQpU+RwOLR7925Pl5Krzp07p2HDhikqKkpeXl5q27atp0vKUwX597h79245HA5NmTLF06UAAHDNI5QCAABXrUWLFipUqJAOHz6caVpSUpJKliypRo0aKT093QPVed5HH32k1157TR06dNDUqVP1+OOPe7qkfGvBggUaNWpUno8zY8YMvfnmm3k+DgAAyB6hFAAAuGoTJ05UampqlmHLM888o6NHj+r999+Xl9f1+dbju+++U+nSpTVu3Dh1795dsbGxni4p31qwYIFGjx6d5+NkF0qVK1dOp0+fVvfu3fO8BgAArnfX5ztDAACQq8qXL6+RI0fqv//9rxYtWuRqX7NmjSZNmqQnnnhCtWvX9mCFeevUqVMXnZ6QkKCIiIhLzufcuXNKTU3NpapwJRwOhwICAuTt7e3pUgAAuOYRSgEAgFzxxBNPqFatWurXr5/OnDmjtLQ0PfLIIypXrpxGjhyp7777TrfeequCg4MVERGhNm3aaOvWrTma98SJE1W9enX5+/urVKlS6t+/vxITEy/5ulGjRsnhcOi3335Tx44dFRYWpiJFimjQoEE6c+ZMpv6ffPKJ6tevr8DAQBUuXFidOnXSvn373Po0bdpUNWrU0C+//KLbbrtNQUFBeuaZZ7IcP+P+REuXLtXmzZvlcDjkcDi0bNky17TXX39db775pmJiYuTv768tW7YoNTVVI0aMUP369RUeHq7g4GDdeuutWrp0aZbzf/311/XOO++oQoUKCgoKUvPmzbVv3z4ZY/Tiiy+qTJkyCgwMVJs2bXT8+PFMdX799deu301oaKhatmypzZs3X3L9StLmzZt1xx13KDAwUGXKlNFLL72U7WWalxqnV69eeueddyTJta4cDodrenp6ut58801Vr15dAQEBKl68uPr27asTJ05kOVZsbKxCQ0MVFhamhg0basaMGZKcv8P58+drz549rjGio6Pd1umF95TKyfabsb3t2LFDvXr1UkREhMLDw/XAAw9kCi4XL16sW265RREREQoJCVHlypWz3Y4AALhW+Xi6AAAAcG3w8fHR+++/r5tvvlkvvviiIiMjtXbtWi1cuFD/93//pxYtWqhChQoaNWqUTp8+rQkTJqhJkyZau3atKxDIyqhRozR69Gg1a9ZMjz76qLZt26Z3331Xa9as0fLly+Xr63vJ2jp27Kjo6GjFx8dr5cqVGj9+vE6cOKGPP/7Y1WfMmDF6/vnn1bFjRz344IM6cuSIJkyYoNtuu03r1q1zO9Pp2LFjatGihTp16qRu3bqpePHiWY5brFgxTZs2TWPGjFFKSori4+MlSVWrVtXp06clSZMnT9aZM2f08MMPy9/fX4ULF1ZycrI++OADde7cWQ899JBOnjypDz/8UHFxcVq9erXq1KnjNs706dOVmpqqxx57TMePH9err76qjh076o477tCyZcv01FNPaceOHZowYYKGDBmijz76yPXaadOmqWfPnoqLi9PYsWN16tQpvfvuu7rlllu0bt26i/5uDh06pNtvv13nzp3T008/reDgYL3//vsKDAzM1Dcn4/Tt21cHDhzQ4sWLNW3atEzz6Nu3r6ZMmaIHHnhAAwcO1K5du/T2229r3bp1btvClClT1Lt3b1WvXl3Dhw9XRESE1q1bp4ULF6pLly569tlnlZSUpD///FPjxo2TJIWEhGS7nEuWLLms7bdjx44qX7684uPjtXbtWn3wwQeKjIzU2LFjJTmDvFatWqlWrVp64YUX5O/vrx07dmj58uXZ1gAAwDXJAAAA5KIBAwYYX19fExISYjp37myMMaZOnTomMjLSHDt2zNVvw4YNxsvLy/To0cPVNnnyZCPJ7Nq1yxhjTEJCgvHz8zPNmzc3aWlprn5vv/22kWQ++uiji9YycuRII8m0bt3arb1fv35GktmwYYMxxpjdu3cbb29vM2bMGLd+GzduND4+Pm7tsbGxRpKZNGlSjtdJbGysqV69ulvbrl27jCQTFhZmEhIS3KadO3fO/P33325tJ06cMMWLFze9e/fONI9ixYqZxMREV/vw4cONJFO7dm1z9uxZV3vnzp2Nn5+fOXPmjDHGmJMnT5qIiAjz0EMPuY116NAhEx4enqn9QoMHDzaSzKpVq1xtCQkJJjw83O33eDnj9O/f32T1FvXHH380ksz06dPd2hcuXOjWnpiYaEJDQ02jRo3M6dOn3fqmp6e7fm7ZsqUpV65cpnEy1unkyZNdbTndfjO2t/N/R8YY065dO1OkSBHX83HjxhlJ5siRI5nGBwDgesLlewAAIFeNGTNGRYoUkZeXl8aNG6eDBw9q/fr16tWrlwoXLuzqV6tWLd11111asGBBtvNasmSJUlNTNXjwYLebpD/00EMKCwvT/Pnzc1RT//793Z4/9thjkuQae9asWUpPT1fHjh119OhR16NEiRKqVKlSpsvm/P399cADD+Ro7Etp3769ihUr5tbm7e0tPz8/Sc5L1o4fP65z586pQYMGWrt2baZ53H///QoPD3c9b9SokSSpW7du8vHxcWtPTU3V/v37JTkvIUtMTFTnzp3dltvb21uNGjXKtNwXWrBggW666SbdeOONrrZixYqpa9eubv2udhxJmjlzpsLDw3XXXXe5zaN+/foKCQlxzWPx4sU6efKknn76aQUEBLjN4/xLAXPqSrbfRx55xO35rbfeqmPHjik5OVmSXGfdzZkz57r9RkoAACQu3wMAALksLCxMlStX1tGjR1W8eHGtXLlSklS5cuVMfatWrapvvvlGf/31l4KDgzNN37NnT5av9fPzU4UKFVzTL6VSpUpuz2NiYuTl5aXdu3dLkrZv3y5jTKZ+GS68RLB06dKu0ChDUlKS65K8jBrPDzGyU758+Szbp06dqn//+9/67bffdPbs2Yv2L1u2rNvzjIAqKioqy/aMezBt375dknTHHXdkWUNYWNhFa9+zZ48rADvfhb+vqx0nYx5JSUmKjIzMcnpCQoIkaefOnZKkGjVqXHKeOZHdNihlv/1e+PsoVKiQJOd6DwsL07/+9S998MEHevDBB/X000/rzjvv1H333acOHTpct99QCQC4PhFKAQCA686FZ8ykp6fL4XDo66+/zvJb1y6831BW90waNGiQpk6d6noeGxurZcuWXbKWrOb1ySefqFevXmrbtq2GDh2qyMhIeXt7Kz4+3hW6nC+7b4rLrt0YI0mus3SmTZumEiVKZOp3/llWVyM3xklPT1dkZKSmT5+e5fQLzzbzpEut98DAQP3www9aunSp5s+fr4ULF+qzzz7THXfcoUWLFvHNfwCA6wahFAAAyFPlypWTJG3bti3TtN9++01FixbN8iypC19boUIFV3tqaqp27dqlZs2a5aiG7du3u51htGPHDqWnp7tuUB0TEyNjjMqXL68bbrghR/O80LBhw9StWzfX84yzY67EF198oQoVKmjWrFluAdrIkSOveJ5ZiYmJkSRFRkbmeF2er1y5cq6zoM534e/6csbJ7hK7mJgYLVmyRE2aNMkyyLtwrE2bNqlixYqXPc6Frmb7vRgvLy/deeeduvPOO/XGG2/o5Zdf1rPPPqulS5de0e8CAICCiPODAQBAnipZsqTq1KmjqVOnKjEx0dW+adMmLVq0SPfcc0+2r23WrJn8/Pw0fvx411kmkvThhx8qKSlJLVu2zFEN77zzjtvzCRMmSJJatGghSbrvvvvk7e2t0aNHu40jOc9uOXbs2CXHqFatmpo1a+Z61K9fP0e1ZSXjTJnza1m1apVWrFhxxfPMSlxcnMLCwvTyyy+7XSKY4ciRIxd9/T333KOVK1dq9erVbq+58GymyxknI+A5f1uRnN9ol5aWphdffDHT68+dO+fq37x5c4WGhio+Pl5nzpxx63f++gwODlZSUtJFl0+6uu03O8ePH8/UlvGNin///fdlzw8AgIKKM6UAAECee+2119SiRQs1btxYffr00enTpzVhwgSFh4dr1KhR2b6uWLFiGj58uEaPHq27775brVu31rZt2zRx4kQ1bNjQ7cyki9m1a5dat26tu+++WytWrNAnn3yiLl26qHbt2pKcZ9e89NJLGj58uHbv3q22bdsqNDRUu3bt0uzZs/Xwww9ryJAhubEqcqRVq1aaNWuW2rVrp5YtW2rXrl2aNGmSqlWrppSUlFwbJywsTO+++666d++uevXqqVOnTipWrJj27t2r+fPnq0mTJnr77bezff2wYcM0bdo03X333Ro0aJCCg4P1/vvvq1y5cvr111+vaJyMMG/gwIGKi4uTt7e3OnXqpNjYWPXt21fx8fFav369mjdvLl9fX23fvl0zZ87UW2+9pQ4dOigsLEzjxo3Tgw8+qIYNG6pLly4qVKiQNmzYoFOnTrkusaxfv74+++wzPfHEE2rYsKFCQkJ07733ZrmcV7r9ZueFF17QDz/8oJYtW6pcuXJKSEjQxIkTVaZMGd1yyy2XPT8AAAosj33vHwAAuGbFxsaa6tWru7UtWbLENGnSxAQGBpqwsDBz7733mi1btrj1mTx5spFkdu3a5db+9ttvmypVqhhfX19TvHhx8+ijj5oTJ05cso6RI0caSWbLli2mQ4cOJjQ01BQqVMgMGDDAnD59OlP/L7/80txyyy0mODjYBAcHmypVqpj+/fubbdu2XXTZLiWr1+zatctIMq+99lqm/unp6ebll1825cqVM/7+/qZu3bpm3rx5pmfPnqZcuXKXnMfSpUuNJDNz5ky39oz1u2bNmkz94+LiTHh4uAkICDAxMTGmV69e5ueff77ksv36668mNjbWBAQEmNKlS5sXX3zRfPjhh1n+HnMyzrlz58xjjz1mihUrZhwOh7nw7er7779v6tevbwIDA01oaKipWbOmGTZsmDlw4IBbv7lz55qbb77Ztb3deOON5r///a9rekpKiunSpYuJiIgwklzrNWOdTp482W1+Odl+M7a3I0eOZLneM9bHt99+a9q0aWNKlSpl/Pz8TKlSpUznzp3N77//fsn1DQDAtcRhzAXnqAMAAFwjRo0apdGjR+vIkSMqWrSop8sBAADAebinFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjntKAQAAAAAAwDrOlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1vl4ugDkjvT0dB04cEChoaFyOByeLgcAAAAAABQQxhidPHlSpUqVkpeXvfOXCKWuEQcOHFBUVJSnywAAAAAAAAXUvn37VKZMGWvjEUpdI0JDQyU5N6CwsDAPVwMAAAAAAAqK5ORkRUVFubIFWwilrhEZl+yFhYURSgEAAAAAgMtm+3ZA3OgcAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWOfj6QLyi6ZNm6pOnTp68803r3gehw4dUvfu3fV///d/8vX1VWJiohwOh2bPnq22bdvmWq3XiunTpXfekY4fdz6PiJA6dpSeeEIaPFj6/nvp1CnntJgY6fHHpbvuyrt6kpKkUaOkOXMkPz/3aamp0v790tmzkre3dO5c5tf7+Ejp6c5HfuTllXVtXl6SMc7H9czfXypVSvL1dW8/etS53ooWdf7uz7dv3z/bKPKOj4/z93L69D9tGdtzYKCUlubcRzPar8Xt2cvLeYwsWjTzNBvHRxQ8e/dKkyZJv/4qBQRImzb9M+3UKeexzeFw/j3L2H9w/fHzk4oXdx5Lz5yRjh2T/vrLOS001HnM8fNzHn9PnXL+rUxPd07L4OXl3MYOHXJODwhwvqZhQyk6Wnr9denkSSk8XIqMlIKDpUcfdbZNmyaFhTmPYZ5+q5yU5KypTJnM0/7807nM4eH26wJsuPBz2fkOHpRSUvL2M463t/M4FBzsPKZkHGN8faUiRaRKlZx/z3x9peHDpVat/nntlCnOY0mFCtKgQVKNGnlXJ/KGw5hr4627MUZ33XWXvL299c0337hNmzhxop555hlt2rRJZbL6S6PcCaWeeuopzZ8/X7Nnz1Z4eLgiIyN16NAhFSpUSP7+/lc835xITk5WeHi4kpKSFBYWlqdj5Ybp06Vu3bKe1rixtGJF1tMWLcqbD15JSVLVqs6DLgAURHl1fETBs3evM6zM6j9QgPxq9mzPBVNJSdLdd0sJCdKyZVJU1D/T9u2TmjZ1BmoLFxJM4dpzsc9l+dWcOVLr1tKHH0oPPvhPu8PhDK8Ipq6MpzKFa+byPYfDocmTJ2vVqlV67733XO27du3SsGHDNGHChGwDqdyyc+dO1a9fX5UqVVJkZKQkqUSJEhcNpM6ePZunNeVXa9ZkPy27QEqSNm7M/Vok5/+MZfU/AwBQUOTV8REFz+HDBFIoeNau9dzYJ086A6k//nAGUPv2OdszAqk//nBOP3nSczUCeeVin8vyqzZtpMcecw+kJOeZ8rt3e6QkXIVrJpSSpKioKL311lsaMmSIdu3aJWOM+vTpo+bNm6ts2bK68cYb5e/vr5IlS+rpp5/WuYu8Yztx4oR69OihQoUKKSgoSC1atND27duz7R8dHa0vv/xSH3/8sRwOh3r16iXJGZZ99dVXkqTdu3fL4XDos88+U2xsrAICAjR9+nRJ0gcffKCqVasqICBAVapU0cSJE3NtveRHb74pDRhwea959VXnpX15oUwZafv2zJftAUBBkJfHRxQ8DRtKK1d6ugog5555RnrhBc+NX6aM8wypChX+Cab+7//+CaQqVHBOz+P/3wY84ko+l+UHb7+duW3OHPdL+1BAmGtQmzZtTNOmTc348eNNsWLFzO7du01QUJDp16+f2bp1q5k9e7YpWrSoGTlypOs1sbGxZtCgQa7nrVu3NlWrVjU//PCDWb9+vYmLizMVK1Y0qampWY6ZkJBg7r77btOxY0dz8OBBk5iYaIwxRpKZPXu2McaYXbt2GUkmOjrafPnll+aPP/4wBw4cMJ988okpWbKkq+3LL780hQsXNlOmTMl2Gc+cOWOSkpJcj3379hlJJikp6arXn00DBmTc/eXij1dftVPP3r3G+PnlrCYe18cjKMiYe+7xfB08eGT3sHV8RMGzcqXnt08ePC71eOYZT+8p/9i715gKFdzrq1DB2Q5c63L6ucxTj0vVN2eOp9dgwZeUlGQ8kSnI6miWHD582BQtWtR4eXmZ2bNnm2eeecZUrlzZpKenu/q88847JiQkxKSlpRlj3EOp33//3Ugyy5cvd/U/evSoCQwMNJ9//nm247Zp08b07NnTrS2rUOrNN9906xMTE2NmzJjh1vbiiy+axo0bZzvWyJEjjaRMj4IWShljTJUqFz/A3HWX3XqWL/f8QZdH/nlkHAaKF/d8LTx4XPiwfXxEwTNpkue3Ux7X/mPoUOfjcl/XsKGn95DMLnwfeN7HAeCad6nPZZ48xhhjTJcuF5+Oq+OpUOqaunwvQ2RkpPr27auqVauqbdu22rp1qxo3biyHw+Hq06RJE6WkpOjPP//M9PqtW7fKx8dHjRo1crUVKVJElStX1tatW6+6vgYNGrh+/uuvv7Rz50716dNHISEhrsdLL72knTt3ZjuP4cOHKykpyfXYl3HxewHz2GPSb79dvM/ixdJrr9mpZ98+6fbb7YyFguGuu6SWLZ33aAHyG5vHRxQ8q1ZJjzzi6SpwPXjttSs7Fq1ZIz37bO7Xc6X27ZO6d3dv6979n3tMAdeynHwu85TXXnPWN2NG9tPnzrVbE3KPj6cLyCs+Pj7y8cmfixccHOz6OSUlRZL0n//8xy0EkyTvC79//jz+/v55/o1+ee2xx7K+Fjgrw4Y5/x06NO/q2bdPqliRr8aGu1OnpAULPF0FkD0bx0cUPKtWSTfd5OkqgEt7+WXnv2PGeLaO829qXqGC8yvmu3f/5x5TF34rH3AtuZzPZZ5yqfratPnnW/lQsFyTZ0pdqGrVqlqxYoWMMa625cuXKzQ0NMtv5KtatarOnTunVatWudqOHTumbdu2qVq1arlaW/HixVWqVCn98ccfqlixotujfPnyuTpWfjJ48OUf+IYNk954I0/K0Z9/SpUqEUgBKJjy8viIgmfNGgIpFCwvvyyNGOG58f/8M/NNzW++OfPNz7O4wAIo8K7kc1l+kNXN2du0kebNs18Lrs51EUr169dP+/bt02OPPabffvtNc+bM0ciRI/XEE0/IyyvzKqhUqZLatGmjhx56SD/99JM2bNigbt26qXTp0mrTpk2u1zd69GjFx8dr/Pjx+v3337Vx40ZNnjxZb1zDnzAaNsx+WuPG2U+rWTP3a5Gk0FCpcOG8mTcA2JBXx0cUPMWLS/n0ZHEgW/XqeW7s0FApMvKfQCrjjKioqH+CqchIZz/gWnOxz2X51Zw50oQJ0gcfuLc7HFJ0tEdKwlW4Lt6ylC5dWgsWLNDQoUNVu3ZtFS5cWH369NFzzz2X7WsmT56sQYMGqVWrVkpNTdVtt92mBQsWyNfXN9fre/DBBxUUFKTXXntNQ4cOVXBwsGrWrKnBgwfn+lj5Rdeuzn/feUc6ftz5c0SE1LGj82vNBw+Wvv/eeemUJMXESI8/7ry/T14ID5e2bpVGjXIe5Pz83Kenpkr790tnz0re3tK5c5nn4eMjpac7H/mRl1fWtXl5/XObwOuZv79UqpR04S5+9KhzvRUt6vzdn2/fvn+2UeQdHx/n7+X06X/aMrbnwEApLe2fsxyv1e3Zy8t5jCxaNPO0vD4+ouApW1bauVOaNEn69VcpIEDatOmf6adOOY9tDofz7xlnCV+//PycIWZgoHTmjHTsmPTXX85poaHOY46fn/P4e+qU829lerp7OOPl5dzGDh1yTg8IcL6mYUPnh8PXX5dOnnS+14qMlIKDpUcfdbZNmyaFhTmPYW3bemINOIWHSwsXOmu68CKKqCjne9LQUGc/4FqT1eey8x08KKWk5O1nHG9v53EoONh5TMk4xvj6SkWKOK9o+fVX5/Phw6VWrZzT+/RxvnbaNGd4PGiQVKNG3tWJvOEw5lp76359Sk5OVnh4uJKSkhQWFubpcgAAAAAAQAHhqUzhurh8DwAAAAAAAPkLoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAAAAAwDpCKQAAAAAAAFhHKAUAAAAAAADrCKUAAAAAAABgHaEUAAAAAAAArCOUAgAAAAAAgHWEUgAAAAAAALCOUAoAAAAAAADWEUoBAAAAAADAOkIpAAAAAAAAWEcoBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwzsfTBSB3GGMkScnJyR6uBAAAAAAAFCQZWUJGtmALodQ14uTJk5KkqKgoD1cCAAAAAAAKopMnTyo8PNzaeA5jOwZDnkhPT9eBAwcUGhoqh8Ph6XIuS3JysqKiorRv3z6FhYV5uhwg32DfALLGvgFkj/0DyBr7BpC1jH1j7969cjgcKlWqlLy87N3piTOlrhFeXl4qU6aMp8u4KmFhYfyBALLAvgFkjX0DyB77B5A19g0ga+Hh4R7ZN7jROQAAAAAAAKwjlAIAAAAAAIB1hFLwOH9/f40cOVL+/v6eLgXIV9g3gKyxbwDZY/8Assa+AWTN0/sGNzoHAAAAAACAdZwpBQAAAAAAAOsIpQAAAAAAAGAdoRQAAAAAAACsI5SCx73zzjuKjo5WQECAGjVqpNWrV3u6JCDXxMfHq2HDhgoNDVVkZKTatm2rbdu2ufU5c+aM+vfvryJFiigkJETt27fX4cOH3frs3btXLVu2VFBQkCIjIzV06FCdO3fOrc+yZctUr149+fv7q2LFipoyZUpeLx6Qa1555RU5HA4NHjzY1ca+gevV/v371a1bNxUpUkSBgYGqWbOmfv75Z9d0Y4xGjBihkiVLKjAwUM2aNdP27dvd5nH8+HF17dpVYWFhioiIUJ8+fZSSkuLW59dff9Wtt96qgIAARUVF6dVXX7WyfMCVSEtL0/PPP6/y5csrMDBQMTExevHFF3X+LZLZN3A9+OGHH3TvvfeqVKlScjgc+uqrr9ym29wPZs6cqSpVqiggIEA1a9bUggULLn+BDOBBn376qfHz8zMfffSR2bx5s3nooYdMRESEOXz4sKdLA3JFXFycmTx5stm0aZNZv369ueeee0zZsmVNSkqKq88jjzxioqKizLfffmt+/vlnc9NNN5mbb77ZNf3cuXOmRo0aplmzZmbdunVmwYIFpmjRomb48OGuPn/88YcJCgoyTzzxhNmyZYuZMGGC8fb2NgsXLrS6vMCVWL16tYmOjja1atUygwYNcrWzb+B6dPz4cVOuXDnTq1cvs2rVKvPHH3+Yb775xuzYscPV55VXXjHh4eHmq6++Mhs2bDCtW7c25cuXN6dPn3b1ufvuu03t2rXNypUrzY8//mgqVqxoOnfu7JqelJRkihcvbrp27Wo2bdpk/vvf/5rAwEDz3nvvWV1eIKfGjBljihQpYubNm2d27dplZs6caUJCQsxbb73l6sO+gevBggULzLPPPmtmzZplJJnZs2e7Tbe1Hyxfvtx4e3ubV1991WzZssU899xzxtfX12zcuPGylodQCh514403mv79+7uep6WlmVKlSpn4+HgPVgXknYSEBCPJfP/998YYYxITE42vr6+ZOXOmq8/WrVuNJLNixQpjjPMPj5eXlzl06JCrz7vvvmvCwsLM33//bYwxZtiwYaZ69epuY/3rX/8ycXFxeb1IwFU5efKkqVSpklm8eLGJjY11hVLsG7hePfXUU+aWW27Jdnp6eropUaKEee2111xtiYmJxt/f3/z3v/81xhizZcsWI8msWbPG1efrr782DofD7N+/3xhjzMSJE02hQoVc+0rG2JUrV87tRQJyRcuWLU3v3r3d2u677z7TtWtXYwz7Bq5PF4ZSNveDjh07mpYtW7rV06hRI9O3b9/LWgYu34PHpKam6pdfflGzZs1cbV5eXmrWrJlWrFjhwcqAvJOUlCRJKly4sCTpl19+0dmzZ932gypVqqhs2bKu/WDFihWqWbOmihcv7uoTFxen5ORkbd682dXn/Hlk9GFfQn7Xv39/tWzZMtP2y76B69XcuXPVoEED3X///YqMjFTdunX1n//8xzV9165dOnTokNt2HR4erkaNGrntGxEREWrQoIGrT7NmzeTl5aVVq1a5+tx2223y8/Nz9YmLi9O2bdt04sSJvF5M4LLdfPPN+vbbb/X7779LkjZs2KCffvpJLVq0kMS+AUh294Pceo9FKAWPOXr0qNLS0tw+TEhS8eLFdejQIQ9VBeSd9PR0DR48WE2aNFGNGjUkSYcOHZKfn58iIiLc+p6/Hxw6dCjL/SRj2sX6JCcn6/Tp03mxOMBV+/TTT7V27VrFx8dnmsa+gevVH3/8oXfffVeVKlXSN998o0cffVQDBw7U1KlTJf2zbV/s/dOhQ4cUGRnpNt3Hx0eFCxe+rP0HyE+efvppderUSVWqVJGvr6/q1q2rwYMHq2vXrpLYNwDJ7n6QXZ/L3U98Lqs3AOCK9e/fX5s2bdJPP/3k6VIAj9u3b58GDRqkxYsXKyAgwNPlAPlGenq6GjRooJdfflmSVLduXW3atEmTJk1Sz549PVwd4Dmff/65pk+frhkzZqh69epav369Bg8erFKlSrFvAAUYZ0rBY4oWLSpvb+9M36R0+PBhlShRwkNVAXljwIABmjdvnpYuXaoyZcq42kuUKKHU1FQlJia69T9/PyhRokSW+0nGtIv1CQsLU2BgYG4vDnDVfvnlFyUkJKhevXry8fGRj4+Pvv/+e40fP14+Pj4qXrw4+wauSyVLllS1atXc2qpWraq9e/dK+mfbvtj7pxIlSighIcFt+rlz53T8+PHL2n+A/GTo0KGus6Vq1qyp7t276/HHH3edbcu+AdjdD7Lrc7n7CaEUPMbPz0/169fXt99+62pLT0/Xt99+q8aNG3uwMiD3GGM0YMAAzZ49W999953Kly/vNr1+/fry9fV12w+2bdumvXv3uvaDxo0ba+PGjW5/PBYvXqywsDDXB5fGjRu7zSOjD/sS8qs777xTGzdu1Pr1612PBg0aqGvXrq6f2TdwPWrSpIm2bdvm1vb777+rXLlykqTy5curRIkSbtt1cnKyVq1a5bZvJCYm6pdffnH1+e6775Senq5GjRq5+vzwww86e/asq8/ixYtVuXJlFSpUKM+WD7hSp06dkpeX+8dXb29vpaenS2LfACS7+0Guvce6rNuiA7ns008/Nf7+/mbKlClmy5Yt5uGHHzYRERFu36QEFGSPPvqoCQ8PN8uWLTMHDx50PU6dOuXq88gjj5iyZcua7777zvz888+mcePGpnHjxq7pGV9737x5c7N+/XqzcOFCU6xYsSy/9n7o0KFm69at5p133uFr71HgnP/te8awb+D6tHr1auPj42PGjBljtm/fbqZPn26CgoLMJ5984urzyiuvmIiICDNnzhzz66+/mjZt2mT5dd9169Y1q1atMj/99JOpVKmS29d9JyYmmuLFi5vu3bubTZs2mU8//dQEBQXxtffIt3r27GlKly5t5s2bZ3bt2mVmzZplihYtaoYNG+bqw76B68HJkyfNunXrzLp164wk88Ybb5h169aZPXv2GGPs7QfLly83Pj4+5vXXXzdbt241I0eONL6+vmbjxo2XtTyEUvC4CRMmmLJlyxo/Pz9z4403mpUrV3q6JCDXSMryMXnyZFef06dPm379+plChQqZoKAg065dO3Pw4EG3+ezevdu0aNHCBAYGmqJFi5onn3zSnD171q3P0qVLTZ06dYyfn5+pUKGC2xhAQXBhKMW+gevV//73P1OjRg3j7+9vqlSpYt5//3236enp6eb55583xYsXN/7+/ubOO+8027Ztc+tz7Ngx07lzZxMSEmLCwsLMAw88YE6ePOnWZ8OGDeaWW24x/v7+pnTp0uaVV17J82UDrlRycrIZNGiQKVu2rAkICDAVKlQwzz77rNtX1rNv4HqwdOnSLD9f9OzZ0xhjdz/4/PPPzQ033GD8/PxM9erVzfz58y97eRzGGHN551YBAAAAAAAAV4d7SgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACwjlAKAAAAAAAA1hFKAQAAIM9MmTJFERERni4DAADkQ4RSAAAAF9GrVy85HI5Mjx07dni6tByJjo6Ww+HQypUr3doHDx6spk2beqYoAAAAEUoBAABc0t13362DBw+6PcqXL5+pX2pqqgequ7SAgAA99dRTni4jV509e9bTJQAAgKtEKAUAAHAJ/v7+KlGihNvD29tbTZs21YABAzR48GAVLVpUcXFxkqQ33nhDNWvWVHBwsKKiotSvXz+lpKS45pdxSdu8efNUuXJlBQUFqUOHDjp16pSmTp2q6OhoFSpUSAMHDlRaWprrdX///beGDBmi0qVLKzg4WI0aNdKyZcsuWf/DDz+slStXasGCBdn2adq0qQYPHuzW1rZtW/Xq1cv1PDo6Wi+99JJ69OihkJAQlStXTnPnztWRI0fUpk0bhYSEqFatWvr5558zzf+rr75SpUqVFBAQoLi4OO3bt89t+pw5c1SvXj0FBASoQoUKGj16tM6dO+ea7nA49O6776p169YKDg7WmDFjLrncAAAgfyOUAgAAuApTp06Vn5+fli9frkmTJkmSvLy8NH78eG3evFlTp07Vd999p2HDhrm97tSpUxo/frw+/fRTLVy4UMuWLVO7du20YMECLViwQNOmTdN7772nL774wvWaAQMGaMWKFfr000/166+/6v7779fdd9+t7du3X7TG8uXL65FHHtHw4cOVnp5+Vcs7btw4NWnSROvWrVPLli3VvXt39ejRQ926ddPatWsVExOjHj16yBjjtqxjxozRxx9/rOXLlysxMVGdOnVyTf/xxx/Vo0cPDRo0SFu2bNF7772nKVOmZAqeRo0apXbt2mnjxo3q3bv3VS0HAADIBwwAAACy1bNnT+Pt7W2Cg4Ndjw4dOhhjjImNjTV169a95DxmzpxpihQp4no+efJkI8ns2LHD1da3b18TFBRkTp486WqLi4szffv2NcYYs2fPHuPt7W3279/vNu8777zTDB8+PNuxy5UrZ8aNG2cSEhJMaGio+fjjj40xxgwaNMjExsa6+sXGxppBgwa5vbZNmzamZ8+ebvPq1q2b6/nBgweNJPP888+72lasWGEkmYMHD7ot68qVK119tm7daiSZVatWuZbh5Zdfdht72rRppmTJkq7nkszgwYOzXU4AAFDw+HgyEAMAACgIbr/9dr377ruu58HBwa6f69evn6n/kiVLFB8fr99++03Jyck6d+6czpw5o1OnTikoKEiSFBQUpJiYGNdrihcvrujoaIWEhLi1JSQkSJI2btyotLQ03XDDDW5j/f333ypSpMgll6FYsWIaMmSIRowYoX/96185XPLMatWq5VafJNWsWTNTW0JCgkqUKCFJ8vHxUcOGDV19qlSpooiICG3dulU33nijNmzYoOXLl7udGZWWlpZpnTVo0OCK6wYAAPkPoRQAAMAlBAcHq2LFitlOO9/u3bvVqlUrPfrooxozZowKFy6sn376SX369FFqaqorYPH19XV7ncPhyLIt43K7lJQUeXt765dffpG3t7dbv/ODrIt54oknNHHiRE2cODHTNC8vL7dL7qSsbyZ+fo0OhyPbtsu5TDAlJUWjR4/Wfffdl2laQECA6+cL1zUAACjYCKUAAABy0S+//KL09HT9+9//lpeX8/adn3/++VXPt27dukpLS1NCQoJuvfXWK5pHSEiInn/+eY0aNUqtW7d2m1asWDEdPHjQ9TwtLU2bNm3S7bffflV1S9K5c+f0888/68Ybb5Qkbdu2TYmJiapataokqV69etq2bVu2wR8AALg2caNzAACAXFSxYkWdPXtWEyZM0B9//KFp06a5boB+NW644QZ17dpVPXr00KxZs7Rr1y6tXr1a8fHxmj9/fo7n8/DDDys8PFwzZsxwa7/jjjs0f/58zZ8/X7/99pseffRRJSYmXnXdkvNMqscee0yrVq3SL7/8ol69eummm25yhVQjRozQxx9/rNGjR2vz5s3aunWrPv30Uz333HO5Mj4AAMifCKUAAAByUe3atfXGG29o7NixqlGjhqZPn674+PhcmffkyZPVo0cPPfnkk6pcubLatm2rNWvWqGzZsjmeh6+vr1588UWdOXPGrb13797q2bOnevToodjYWFWoUCFXzpKSnPfPeuqpp9SlSxc1adJEISEh+uyzz1zT4+LiNG/ePC1atEgNGzbUTTfdpHHjxqlcuXK5Mj4AAMifHObCmwcAAAAAAAAAeYwzpQAAAAAAAGAdoRQAAAAAAACsI5QCAAAAAACAdYRSAAAAAAAAsI5QCgAAAAAAANYRSgEAAAAAAMA6QikAAAAAAABYRygFAAAAAAAA6wilAAAAAAAAYB2hFAAAAAAAAKwjlAIAAAAAAIB1hFIAAAAAAACw7v8BILdxUW0KqWwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Referred to documentation to help teach me about all of this\n",
        "# https://www.tensorflow.org/guide/keras/functional#convert_a_sequential_model_to_a_functional_model\n",
        "# https://keras.io/examples/vision/grad_cam/\n",
        "# https://docs.opencv.org/4.x/dd/d9e/classcv_1_1VideoWriter.html\n",
        "\n",
        "sequential_model = load_model('fire_detection_cnn.keras')\n",
        "\n",
        "# Using the last Conv2D layer\n",
        "layer_name = 'conv2d_8'\n",
        "\n",
        "# Building model to take frames of 150x150 that gets the activation of Conv2D and the prediction\n",
        "inp = Input(shape=(150,150,3))\n",
        "x = inp\n",
        "conv_output = None\n",
        "for layer in sequential_model.layers:\n",
        "    x = layer(x)\n",
        "    if layer.name == layer_name:\n",
        "        conv_output = x\n",
        "preds = x\n",
        "\n",
        "# Making grad-CAM model\n",
        "grad_model = Model(inputs=inp, outputs=[conv_output, preds])\n",
        "\n",
        "# Upload video/open it\n",
        "uploaded = files.upload()\n",
        "video_path = next(iter(uploaded.keys()))\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Frames per second\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "# Width and height of frames\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "# Needed to use codec to make file smaller in order to work and be able to store the video because it is too large\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "# Output w/ file name\n",
        "out = cv2.VideoWriter('cnn_gradcam_output.mp4', fourcc, fps, (w, h))\n",
        "\n",
        "# Loops through every frame\n",
        "frame_count = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret: break\n",
        "    frame_count += 1\n",
        "\n",
        "    # Preprocessing\n",
        "    # Resize/Normalize\n",
        "    small = cv2.resize(frame, (150,150)) / 255.0\n",
        "    # Add batch dimension\n",
        "    inp_arr = np.expand_dims(small, 0).astype(np.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Recording sensor readings/fire probability\n",
        "        conv_maps, pred = grad_model(inp_arr)\n",
        "        # Watches them (Sees how changing maps affects the final score)\n",
        "        tape.watch(conv_maps)\n",
        "        # Loss is fire prob\n",
        "        loss = pred[:,0]\n",
        "    # Computes Gradient\n",
        "    grads = tape.gradient(loss, conv_maps)\n",
        "\n",
        "    # Building CAM\n",
        "    weights = tf.reduce_mean(grads, axis=(0,1,2))\n",
        "    cam = tf.reduce_sum(conv_maps[0] * weights, axis=-1)\n",
        "\n",
        "    # Normalize CAM, wanting to make sure it is between 0 and 1 so its not negative number\n",
        "    cam = tf.maximum(cam, 0)\n",
        "    max_val = tf.reduce_max(cam)\n",
        "    if max_val > 0:\n",
        "        cam = cam / max_val\n",
        "    else:\n",
        "        cam = tf.zeros_like(cam)\n",
        "    score = float(pred[0][0])\n",
        "\n",
        "    # Overlay heatmap when model detects fire (greater than 50%)\n",
        "    if score > 0.5:\n",
        "        heat = (cam.numpy()*255).astype(np.uint8)\n",
        "        heat = cv2.resize(heat, (w, h))\n",
        "        heat = cv2.applyColorMap(heat, cv2.COLORMAP_HOT)\n",
        "        frame = cv2.addWeighted(frame, 0.6, heat, 0.4, 0)\n",
        "\n",
        "    # Annotate and write to the frames\n",
        "    color = (0,0,255) if score>0.7 else (255,0,0)\n",
        "    cv2.putText(frame, f\"Fire: {score:.2f}\", (10,30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"Processed {frame_count} frames and saved as cnn_gradcam_output.mp4\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "shutil.copy('cnn_gradcam_output.mp4', '/content/drive/MyDrive/cnn_gradcam_output.mp4')\n",
        "print(\"Also saved to Google Drive at MyDrive/cnn_gradcam_output.mp4\")"
      ],
      "metadata": {
        "id": "jWjQ8_otTVdq",
        "outputId": "86a7170e-18de-42e8-b85f-e47ff4cabd6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b65e1ec-d508-4e6d-8bcb-9c75b2266f8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b65e1ec-d508-4e6d-8bcb-9c75b2266f8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving YosemiteForestFire.mp4 to YosemiteForestFire (1).mp4\n",
            "Processed 9548 frames and saved as cnn_gradcam_output.mp4\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Also saved to Google Drive at MyDrive/cnn_gradcam_output.mp4\n"
          ]
        }
      ]
    }
  ]
}