{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPja3YmCAHnKQOJLN3384ld",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterRoumeliotis/AIFireSmokeDetectionResearchProject/blob/main/AIFireAndSmokeDetectionResearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Peter Roumeliotis**"
      ],
      "metadata": {
        "id": "KULzVLqSgqod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Dense, Dropout, Input\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import kagglehub\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zvi-1Xsl4HBM",
        "outputId": "c60899e3-7e5a-45c8-b981-b98e97cef926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading latest version of the dataset I am using\n",
        "path = kagglehub.dataset_download(\"phylake1337/fire-dataset\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa7tmbqc0Q8L",
        "outputId": "87f866ef-b11d-45ad-e8fa-060066774fda"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/fire-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "\n",
        "# Getting the source paths\n",
        "source_dir = '/kaggle/input/fire-dataset/fire_dataset'\n",
        "source_fire_dir = os.path.join(source_dir, 'fire_images')\n",
        "source_non_fire_dir = os.path.join(source_dir, 'non_fire_images')\n",
        "\n",
        "# Creating training and validation paths\n",
        "dest_dir = 'data'\n",
        "train_fire_dir = os.path.join(dest_dir, 'train', 'fire')\n",
        "val_fire_dir = os.path.join(dest_dir, 'validation', 'fire')\n",
        "train_non_fire_dir = os.path.join(dest_dir, 'train', 'non_fire')\n",
        "val_non_fire_dir = os.path.join(dest_dir, 'validation', 'non_fire')\n",
        "\n",
        "# Making sure the directories exist and if they dont, making them\n",
        "os.makedirs(train_fire_dir, exist_ok=True)\n",
        "os.makedirs(val_fire_dir, exist_ok=True)\n",
        "os.makedirs(train_non_fire_dir, exist_ok=True)\n",
        "os.makedirs(val_non_fire_dir, exist_ok=True)\n",
        "\n",
        "# 80% training 20% validation\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Splitting the files into each folder\n",
        "def split_data(source_folder, train_folder, val_folder, split_ratio=0.8):\n",
        "\n",
        "    # List all items in source folder and keep only files\n",
        "    file_list = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
        "    # Randomizes it\n",
        "    random.shuffle(file_list)\n",
        "\n",
        "    # Figures out at what index to split the files\n",
        "    split_point = int(len(file_list) * split_ratio)\n",
        "    train_files = file_list[:split_point]   # Training\n",
        "    val_files = file_list[split_point:]   # Validation\n",
        "\n",
        "    # Copying files into training\n",
        "    for file_name in train_files:\n",
        "        src = os.path.join(source_folder, file_name)\n",
        "        dst = os.path.join(train_folder, file_name)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "    # Copying files into validation\n",
        "    for file_name in val_files:\n",
        "        src = os.path.join(source_folder, file_name)\n",
        "        dst = os.path.join(val_folder, file_name)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "split_data(source_fire_dir, train_fire_dir, val_fire_dir, split_ratio)\n",
        "split_data(source_non_fire_dir, train_non_fire_dir, val_non_fire_dir, split_ratio)\n",
        "\n",
        "print(\"Data is split\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-dze51z4LiM",
        "outputId": "83031100-21bd-4416-c3fa-b430104c7d60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is split\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 150, 150\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "train_data_dir = 'data/train'\n",
        "validation_data_dir = 'data/validation'\n",
        "\n",
        "# Augmenting training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Rescaling validation data\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Generate validation batches\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjOVd_4N4Xtx",
        "outputId": "456bdb07-8909-45a3-a8ee-5b7e03efd9c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 799 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Model"
      ],
      "metadata": {
        "id": "CpBTYw6NzERN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "QMGJigXYwhac",
        "outputId": "a7398637-ac24-4a99-c1ad-8370cb6d312c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,785\u001b[0m (432.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,785</span> (432.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,337\u001b[0m (431.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,337</span> (431.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator, steps_per_epoch=train_generator.samples, epochs=epochs, validation_data=validation_generator, validation_steps=validation_generator.samples // batch_size)\n",
        "\n",
        "model_save_path = \"fire_detection_cnn.keras\"\n",
        "model_googledrive_path = \"/content/drive/MyDrive/fire_detection_cnn.keras\"\n",
        "model.save(model_googledrive_path)\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved to {model_save_path} and {model_googledrive_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg-TpDsn4fqE",
        "outputId": "9a2a8457-906f-43ee-c392-e93aad415e40"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m 25/799\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12:05\u001b[0m 937ms/step - accuracy: 0.7634 - loss: 0.5274"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 37ms/step - accuracy: 0.8507 - loss: 0.3727 - val_accuracy: 0.9010 - val_loss: 0.6345\n",
            "Epoch 2/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 25ms/step - accuracy: 0.9087 - loss: 0.2213 - val_accuracy: 0.8542 - val_loss: 0.5421\n",
            "Epoch 3/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9168 - loss: 0.1983 - val_accuracy: 0.9062 - val_loss: 0.5307\n",
            "Epoch 4/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9339 - loss: 0.1577 - val_accuracy: 0.8698 - val_loss: 0.4677\n",
            "Epoch 5/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - accuracy: 0.9216 - loss: 0.1848 - val_accuracy: 0.9115 - val_loss: 0.4077\n",
            "Epoch 6/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9438 - loss: 0.1543 - val_accuracy: 0.7969 - val_loss: 0.3883\n",
            "Epoch 7/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 48ms/step - accuracy: 0.9349 - loss: 0.1518 - val_accuracy: 0.8333 - val_loss: 0.3713\n",
            "Epoch 8/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - accuracy: 0.9413 - loss: 0.1565 - val_accuracy: 0.7656 - val_loss: 0.4277\n",
            "Epoch 9/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9252 - loss: 0.1683 - val_accuracy: 0.8490 - val_loss: 0.4362\n",
            "Epoch 10/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9373 - loss: 0.1593 - val_accuracy: 0.8646 - val_loss: 0.3564\n",
            "Epoch 11/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9346 - loss: 0.1487 - val_accuracy: 0.8385 - val_loss: 0.3860\n",
            "Epoch 12/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - accuracy: 0.9422 - loss: 0.1410 - val_accuracy: 0.8021 - val_loss: 0.4265\n",
            "Epoch 13/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 76ms/step - accuracy: 0.9413 - loss: 0.1356 - val_accuracy: 0.8698 - val_loss: 0.3247\n",
            "Epoch 14/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - accuracy: 0.9400 - loss: 0.1579 - val_accuracy: 0.7292 - val_loss: 0.5299\n",
            "Epoch 15/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9433 - loss: 0.1438 - val_accuracy: 0.7240 - val_loss: 0.5845\n",
            "Epoch 16/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9414 - loss: 0.1513 - val_accuracy: 0.6562 - val_loss: 0.7523\n",
            "Epoch 17/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9385 - loss: 0.1498 - val_accuracy: 0.9010 - val_loss: 0.2769\n",
            "Epoch 18/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9397 - loss: 0.1386 - val_accuracy: 0.9323 - val_loss: 0.2215\n",
            "Epoch 19/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.9487 - loss: 0.1270 - val_accuracy: 0.9115 - val_loss: 0.2167\n",
            "Epoch 20/20\n",
            "\u001b[1m799/799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 24ms/step - accuracy: 0.9423 - loss: 0.1182 - val_accuracy: 0.8958 - val_loss: 0.2644\n",
            "Model saved to fire_detection_cnn.keras and /content/drive/MyDrive/fire_detection_cnn.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics --quiet"
      ],
      "metadata": {
        "id": "kPIEF1DJv4bw",
        "outputId": "e9ebea88-7f07-4e58-adee-659462993915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/\n",
        "# https://www.digitalocean.com/community/tutorials/train-yolov5-custom-data\n",
        "# Sources I used to help me format my dataset for yolo\n",
        "\n",
        "for split in [\"train\",\"validation\"]:\n",
        "    for cls_idx, cls in enumerate([\"non_fire\",\"fire\"]):\n",
        "        folder = f\"data/{split}/{cls}\"\n",
        "        for img in os.listdir(folder):\n",
        "            # Skips non images\n",
        "            if not img.lower().endswith((\".jpg\",\".png\")): continue\n",
        "            img_path   = os.path.join(folder, img)\n",
        "            label_path = os.path.splitext(img_path)[0] + \".txt\"\n",
        "\n",
        "            if cls == \"fire\":\n",
        "                with open(label_path, \"w\") as f:\n",
        "                    # Formats data for yolo\n",
        "                    f.write(f\"{1} 0.5 0.5 1.0 1.0\\n\")\n",
        "            else:\n",
        "                open(label_path, \"w\").close()\n"
      ],
      "metadata": {
        "id": "8A9iHkLYAb31"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making YAML for yolo\n",
        "\n",
        "# https://docs.ultralytics.com/\n",
        "# https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/\n",
        "\n",
        "# Used these sources for help\n",
        "\n",
        "%%bash\n",
        "cat <<EOF > data/fire_data.yaml\n",
        "train: /content/data/train\n",
        "val:   /content/data/validation\n",
        "\n",
        "names:\n",
        "  0: non_fire\n",
        "  1: fire\n",
        "EOF\n",
        "\n",
        "echo \"Created YAML:\"\n",
        "cat data/fire_data.yaml\n"
      ],
      "metadata": {
        "id": "c3_4wD-twKV4",
        "outputId": "83ee7dc2-9785-4da3-d457-feb49d84287e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created YAML:\n",
            "train: /content/data/train\n",
            "val:   /content/data/validation\n",
            "\n",
            "names:\n",
            "  0: non_fire\n",
            "  1: fire\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "yolo.train(\n",
        "    data='data/fire_data.yaml',\n",
        "    epochs=20,\n",
        "    imgsz=150,\n",
        "    batch=32,\n",
        "    project='yolo-fire',\n",
        "    name='exp'\n",
        ")\n",
        "\n",
        "yolo_model = YOLO('yolo-fire/exp/weights/best.pt')\n",
        "yolo_model.save('/content/drive/MyDrive/best.pt')\n"
      ],
      "metadata": {
        "id": "9f2AG7tcwNnD",
        "outputId": "ba392648-aedd-4d1d-c080-4398797a0353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 404MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.126 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data/fire_data.yaml, degrees=0.0, deterministic=True, device=cuda:0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=150, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=yolo-fire, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=yolo-fire/exp, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 84.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 360MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "WARNING ⚠️ imgsz=[150] must be multiple of max stride 32, updating to [160]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1299.1±1079.6 MB/s, size: 66.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data/train/fire... 798 images, 194 backgrounds, 1 corrupt: 100%|██████████| 799/799 [00:00<00:00, 1103.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/data/train/fire/fire.357.png: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/data/train/fire/fire.576.png: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/data/train/fire/fire.681.png: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/data/train/non_fire/non_fire.189.png: ignoring corrupt image/label: invalid image format GIF. Supported formats are:\n",
            "images: {'png', 'jpg', 'mpo', 'bmp', 'dng', 'jpeg', 'heic', 'tiff', 'pfm', 'tif', 'webp'}\n",
            "videos: {'asf', 'webm', 'mpeg', 'mpg', 'mov', 'ts', 'mp4', 'wmv', 'mkv', 'm4v', 'avi', 'gif'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data/train/fire.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 485.9±85.4 MB/s, size: 299.9 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data/validation/fire... 200 images, 49 backgrounds, 0 corrupt: 100%|██████████| 200/200 [00:00<00:00, 1227.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/data/validation/fire.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to yolo-fire/exp/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 160 train, 160 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1myolo-fire/exp\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/20     0.346G     0.7383      2.439      1.165         70        160: 100%|██████████| 25/25 [00:12<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.874      0.919      0.963      0.916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/20     0.363G      0.415     0.7979     0.9826         74        160: 100%|██████████| 25/25 [00:12<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.635      0.914      0.761      0.574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/20     0.391G      0.369     0.6833     0.9576         70        160: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.953      0.937      0.983      0.954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/20     0.408G     0.3392     0.5962     0.9437         76        160: 100%|██████████| 25/25 [00:13<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.978      0.894       0.98      0.923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/20     0.424G     0.3307     0.5787     0.9406         76        160: 100%|██████████| 25/25 [00:12<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.898      0.818      0.912      0.859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/20     0.441G     0.3022     0.5082     0.9244         79        160: 100%|██████████| 25/25 [00:13<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.982      0.921      0.982      0.964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/20     0.459G     0.2992     0.4709     0.9234         75        160: 100%|██████████| 25/25 [00:12<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.969      0.817      0.927      0.913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/20     0.477G     0.2694     0.4694       0.92         72        160: 100%|██████████| 25/25 [00:12<00:00,  2.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.905      0.885      0.949      0.886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/20     0.492G     0.2557     0.4373     0.9088         69        160: 100%|██████████| 25/25 [00:12<00:00,  2.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.914       0.96      0.972      0.951\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/20      0.51G     0.2366     0.4088     0.9052         74        160: 100%|██████████| 25/25 [00:12<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.912      0.934      0.958      0.947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/20     0.525G     0.2273     0.6916     0.8797         28        160: 100%|██████████| 25/25 [00:15<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.864      0.927      0.869      0.869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/20     0.543G     0.1914     0.3964     0.8642         20        160: 100%|██████████| 25/25 [00:12<00:00,  2.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.987      0.993      0.991      0.972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/20     0.561G     0.1663     0.3085     0.8471         23        160: 100%|██████████| 25/25 [00:12<00:00,  2.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.973          1      0.994      0.993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/20     0.578G     0.1517     0.2911     0.8601         24        160: 100%|██████████| 25/25 [00:13<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.982      0.993      0.993       0.99\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/20     0.594G       0.13     0.2557     0.8466         22        160: 100%|██████████| 25/25 [00:11<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.993      0.987      0.994      0.994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/20     0.611G     0.1221     0.2316     0.8498         22        160: 100%|██████████| 25/25 [00:13<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.993      0.993      0.995      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/20     0.627G     0.1173     0.2191     0.8409         24        160: 100%|██████████| 25/25 [00:12<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.992      0.987      0.995      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/20     0.645G     0.1013      0.193     0.8465         25        160: 100%|██████████| 25/25 [00:12<00:00,  2.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.993      0.993      0.995      0.992\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/20     0.662G    0.09611     0.1787     0.8522         21        160: 100%|██████████| 25/25 [00:13<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.993      0.993      0.995      0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/20      0.68G    0.09009     0.1858     0.8338         27        160: 100%|██████████| 25/25 [00:10<00:00,  2.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.993      0.993      0.994      0.994\n",
            "\n",
            "20 epochs completed in 0.089 hours.\n",
            "Optimizer stripped from yolo-fire/exp/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from yolo-fire/exp/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating yolo-fire/exp/weights/best.pt...\n",
            "Ultralytics 8.3.126 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        151      0.993      0.993      0.995      0.995\n",
            "                  fire        151        151      0.993      0.993      0.995      0.995\n",
            "Speed: 0.0ms preprocess, 0.8ms inference, 0.0ms loss, 3.0ms postprocess per image\n",
            "Results saved to \u001b[1myolo-fire/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = tf.keras.models.load_model('fire_detection_cnn.keras')\n",
        "yolo_model = YOLO('yolo-fire/exp/weights/best.pt')\n",
        "\n",
        "# Upload/open vid\n",
        "uploaded = files.upload()\n",
        "video_path = next(iter(uploaded.keys()))\n",
        "print(f\"Video file: {video_path}\")\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Frames per second, default is 30\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "\n",
        "frame_indices = []  # Frame numbers\n",
        "cnn_flags     = []  # My model\n",
        "yolo_flags    = []  # Yolo\n",
        "\n",
        "frame_idx = 0\n",
        "\n",
        "# Loop through vid by frame\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_idx += 1\n",
        "\n",
        "    # My model detection\n",
        "    # Resize/normalize\n",
        "    img = cv2.resize(frame, (150,150)) / 255.0\n",
        "    # Get the fire probability from my model\n",
        "    p = cnn_model.predict(np.expand_dims(img,0), verbose=0)[0][0]\n",
        "    # Detect if probability > 0.5\n",
        "    cnn_detected = (p > 0.5)\n",
        "\n",
        "    # YOLO detection\n",
        "    # Run yolo\n",
        "    res = yolo_model(frame, verbose=False)[0]\n",
        "    classes = res.boxes.cls.cpu().numpy().astype(int).tolist()\n",
        "    # Detect if there is class 1 (fire)\n",
        "    yolo_detected = (1 in classes)\n",
        "\n",
        "    # Results\n",
        "    frame_indices.append(frame_idx)\n",
        "    cnn_flags.append(cnn_detected)\n",
        "    yolo_flags.append(yolo_detected)\n",
        "\n",
        "cap.release()\n"
      ],
      "metadata": {
        "id": "z2-L6mgayG04",
        "outputId": "c1f488c1-38bf-4f1d-c1c4-95864db46482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-29880a24-a55c-4ea1-8d63-2f423d58b4b5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-29880a24-a55c-4ea1-8d63-2f423d58b4b5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Yosemite Forest Fire Time Lapse and Flyover.mp4 to Yosemite Forest Fire Time Lapse and Flyover.mp4\n",
            "Video file: Yosemite Forest Fire Time Lapse and Flyover.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cnn_frames = [frame_indices[i] for i, flag in enumerate(cnn_flags) if flag]\n",
        "yolo_frames = [frame_indices[i] for i, flag in enumerate(yolo_flags) if flag]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "# My model detections (red circles)\n",
        "ax1.scatter(cnn_frames, [1]*len(cnn_frames),\n",
        "            label='My model detected', marker='o', c='red')\n",
        "ax1.set_yticks([1])\n",
        "ax1.set_yticklabels(['My model fire'])\n",
        "ax1.set_title('My model per‐frame detections')\n",
        "\n",
        "# Yolo detections (blue X’s)\n",
        "ax2.scatter(yolo_frames, [1]*len(yolo_frames),\n",
        "            label='Yolo detected', marker='x', c='blue')\n",
        "ax2.set_yticks([1])\n",
        "ax2.set_yticklabels(['Yolo fire'])\n",
        "ax2.set_title('Yolo per‐frame detections')\n",
        "\n",
        "ax2.set_xlabel('Frame Number')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UMgiH6PfjkaU",
        "outputId": "120f142a-10b2-4d6a-f40c-de8ad3151c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Referred to documentation to help teach me about all of this\n",
        "# https://www.tensorflow.org/guide/keras/functional#convert_a_sequential_model_to_a_functional_model\n",
        "# https://keras.io/examples/vision/grad_cam/\n",
        "# https://docs.opencv.org/4.x/dd/d9e/classcv_1_1VideoWriter.html\n",
        "\n",
        "sequential_model = load_model('fire_detection_cnn.keras')\n",
        "\n",
        "# Using the last Conv2D layer\n",
        "layer_name = 'conv2d_2'\n",
        "\n",
        "# Building model to take frames of 150x150 that gets the activation of Conv2D and the prediction\n",
        "inp = Input(shape=(150,150,3))\n",
        "x = inp\n",
        "conv_output = None\n",
        "for layer in sequential_model.layers:\n",
        "    x = layer(x)\n",
        "    if layer.name == layer_name:\n",
        "        conv_output = x\n",
        "preds = x\n",
        "\n",
        "# Making grad-CAM model\n",
        "grad_model = Model(inputs=inp, outputs=[conv_output, preds])\n",
        "\n",
        "# Upload video/open it\n",
        "uploaded = files.upload()\n",
        "video_path = next(iter(uploaded.keys()))\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Frames per second\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "# Width and height of frames\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "# Needed to use codec to make file smaller in order to work and be able to store the video because it is too large\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "# Output w/ file name\n",
        "out = cv2.VideoWriter('cnn_gradcam_output.mp4', fourcc, fps, (w, h))\n",
        "\n",
        "# Loops through every frame\n",
        "frame_count = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret: break\n",
        "    frame_count += 1\n",
        "\n",
        "    # Preprocessing\n",
        "    # Resize/Normalize\n",
        "    small = cv2.resize(frame, (150,150)) / 255.0\n",
        "    # Add batch dimension\n",
        "    inp_arr = np.expand_dims(small, 0).astype(np.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Recording sensor readings/fire probability\n",
        "        conv_maps, pred = grad_model(inp_arr)\n",
        "        # Watches them (Sees how changing maps affects the final score)\n",
        "        tape.watch(conv_maps)\n",
        "        # Loss is fire prob\n",
        "        loss = pred[:,0]\n",
        "    # Computes Gradient\n",
        "    grads = tape.gradient(loss, conv_maps)\n",
        "\n",
        "    # Building CAM\n",
        "    weights = tf.reduce_mean(grads, axis=(0,1,2))\n",
        "    cam = tf.reduce_sum(conv_maps[0] * weights, axis=-1)\n",
        "\n",
        "    # Normalize CAM, wanting to make sure it is between 0 and 1 so its not negative number\n",
        "    cam = tf.maximum(cam, 0)\n",
        "    max_val = tf.reduce_max(cam)\n",
        "    if max_val > 0:\n",
        "        cam = cam / max_val\n",
        "    else:\n",
        "        cam = tf.zeros_like(cam)\n",
        "    score = float(pred[0][0])\n",
        "\n",
        "    # Overlay heatmap when model detects fire (greater than 50%)\n",
        "    if score > 0.5:\n",
        "        heat = (cam.numpy()*255).astype(np.uint8)\n",
        "        heat = cv2.resize(heat, (w, h))\n",
        "        heat = cv2.applyColorMap(heat, cv2.COLORMAP_HOT)\n",
        "        frame = cv2.addWeighted(frame, 0.6, heat, 0.4, 0)\n",
        "\n",
        "    # Annotate and write to the frames\n",
        "    color = (0,0,255) if score>0.7 else (255,0,0)\n",
        "    cv2.putText(frame, f\"Fire: {score:.2f}\", (10,30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"Processed {frame_count} frames and saved as cnn_gradcam_output.mp4\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "shutil.copy('cnn_gradcam_output.mp4', '/content/drive/MyDrive/cnn_gradcam_output.mp4')\n",
        "print(\"Also saved to Google Drive at MyDrive/cnn_gradcam_output.mp4\")"
      ],
      "metadata": {
        "id": "jWjQ8_otTVdq",
        "outputId": "eb1c348c-ef71-4780-8568-3cccfa507915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3df547f0-6b80-4e1e-8dba-ba0a25dd6270\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3df547f0-6b80-4e1e-8dba-ba0a25dd6270\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Yosemite Forest Fire Time Lapse and Flyover.mp4 to Yosemite Forest Fire Time Lapse and Flyover (6).mp4\n",
            "Processed 9548 frames and saved as cnn_gradcam_output.mp4\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Also saved to Google Drive at MyDrive/cnn_gradcam_output.mp4\n"
          ]
        }
      ]
    }
  ]
}